{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tf-cpu",
   "display_name": "tf-cpu",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions with StatCast\n",
    "\n",
    "Our goal is to make some prediction of subsequent season box score data with StatCast data.\n",
    "\n",
    "## Data Cleaning and Prep first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n"
   ]
  },
  {
   "source": [
    "#### Start with adding some approximate physical quantities\n",
    "\n",
    "We know that we're dealing with physical data, so we introduce a few tirgonometric functions to represent component velocities and some approximate kinematics to represent other features of baseballs in the air.\n",
    "\n",
    "These are definitely \"first approximations.\"\n",
    "\n",
    "Definitions:\n",
    "EV_x: the velocity of the ball in the direction of the fences.\n",
    "EV_y: the velocity of the ball upward.\n",
    "cosLA: cosine of the launch angle\n",
    "sinLA: sine of the launch angle\n",
    "Hangtime: the (approximate) time required for the ball to return to the level of the playing field. Calulate by basic kinematics on EV_y.\n",
    "Distance: This is the approximate distance from homeplate when the ball reaches the level of the playing field. No drag calculations here. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "statcast = pd.read_csv('../data/hitters_statcast_since2016.csv').dropna()\n",
    "statcast['Barrel%'] = statcast['Barrel%'].apply(lambda x: float(x.strip('%')))\n",
    "statcast['HardHit%'] = statcast['HardHit%'].apply(lambda x: float(x.strip('%')))\n",
    "\n",
    "##Introduce a number of physical quantities, very approximate.\n",
    "statcast['cosLA'] = np.cos(statcast['LA']/180*np.pi)\n",
    "statcast['sinLA'] = np.sin(statcast['LA']/180*np.pi)\n",
    "statcast['EV_x'] = statcast['EV']*statcast['cosLA']\n",
    "statcast['EV_y'] = statcast['EV']*statcast['sinLA']\n",
    "acceleration = -32 * 60 * 60 / 5280\n",
    "func = (lambda x: -2 * x / acceleration)\n",
    "statcast['Hangtime'] = func(statcast['EV_y'])\n",
    "statcast['Distance'] = statcast['Hangtime']*statcast['EV_x']\n",
    "\n",
    "## We know that the optimal LA for homeruns is not 0 degrees.\n",
    "## Below, we found that an average LA of 20 was very good for HR\n",
    "## hitting. We score this by the cosine squared.\n",
    "\n",
    "statcast['LA_optimality'] = np.cos((statcast['LA'] - 20)/180*np.pi)**2\n",
    "\n",
    "##In particluar, that LA_optimality times av. exist velocity correlated well with \n",
    "##HR%. So, let's add that too\n",
    "\n",
    "\n",
    "\n",
    "standard = pd.read_csv('../data/hitters_since_1947.csv')\n",
    "standard = standard[standard['Season'] >= 2016]\n",
    "standard.drop(['G','AB','AVG','R','RBI'], axis = 1, inplace=True)\n",
    "\n",
    "statcast_cols = statcast.select_dtypes(exclude = 'object').drop(['playerid','Season'], axis = 1).columns \n",
    "standard_cols = standard.select_dtypes(exclude = 'object').drop(['playerid','Season'], axis = 1).columns\n",
    "\n",
    "df = pd.merge(statcast, standard.select_dtypes(exclude='object'), on = ['playerid','Season'], how = 'left').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Season            Name     Team  PA_x  Events     EV  maxEV    LA  Barrels  \\\n",
       "0    2018     Ryder Jones   Giants     8       3  109.1  112.1  18.3      2.0   \n",
       "1    2016  Eric Young Jr.  Yankees     1       1  107.9  107.9  12.5      0.0   \n",
       "2    2017     Aaron Blair   Braves     2       1  106.8  106.8  -2.5      0.0   \n",
       "3    2017      Erik Kratz  Yankees     2       2  106.0  108.6   5.5      1.0   \n",
       "4    2019    Jesus Tinoco  Rockies     6       2  106.0  106.0 -17.0      0.0   \n",
       "\n",
       "   Barrel%  ...       HR%  BB%  IBB%       SO%  HBP%  SF%  SH%  GDP%  SB%  CS%  \n",
       "0     66.7  ...  0.666667  0.0   0.0  1.666667   0.0  0.0  0.0   0.0  0.0  0.0  \n",
       "1      0.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   0.0  1.0  0.0  \n",
       "2      0.0  ...  0.000000  1.0   0.0  0.000000   0.0  0.0  0.0   0.0  0.0  0.0  \n",
       "3     50.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   0.0  0.0  0.0  \n",
       "4      0.0  ...  0.000000  0.0   0.0  2.000000   0.0  0.0  0.5   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>Name</th>\n      <th>Team</th>\n      <th>PA_x</th>\n      <th>Events</th>\n      <th>EV</th>\n      <th>maxEV</th>\n      <th>LA</th>\n      <th>Barrels</th>\n      <th>Barrel%</th>\n      <th>...</th>\n      <th>HR%</th>\n      <th>BB%</th>\n      <th>IBB%</th>\n      <th>SO%</th>\n      <th>HBP%</th>\n      <th>SF%</th>\n      <th>SH%</th>\n      <th>GDP%</th>\n      <th>SB%</th>\n      <th>CS%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018</td>\n      <td>Ryder Jones</td>\n      <td>Giants</td>\n      <td>8</td>\n      <td>3</td>\n      <td>109.1</td>\n      <td>112.1</td>\n      <td>18.3</td>\n      <td>2.0</td>\n      <td>66.7</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016</td>\n      <td>Eric Young Jr.</td>\n      <td>Yankees</td>\n      <td>1</td>\n      <td>1</td>\n      <td>107.9</td>\n      <td>107.9</td>\n      <td>12.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017</td>\n      <td>Aaron Blair</td>\n      <td>Braves</td>\n      <td>2</td>\n      <td>1</td>\n      <td>106.8</td>\n      <td>106.8</td>\n      <td>-2.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017</td>\n      <td>Erik Kratz</td>\n      <td>Yankees</td>\n      <td>2</td>\n      <td>2</td>\n      <td>106.0</td>\n      <td>108.6</td>\n      <td>5.5</td>\n      <td>1.0</td>\n      <td>50.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019</td>\n      <td>Jesus Tinoco</td>\n      <td>Rockies</td>\n      <td>6</td>\n      <td>2</td>\n      <td>106.0</td>\n      <td>106.0</td>\n      <td>-17.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 50 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.rename({'PA_y':'PA'}, axis=1, inplace = True)\n",
    "for s in standard_cols:\n",
    "    df[s+'%'] = df[s]/df['Events']\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Season            Name       Team  PA_x  Events     EV  maxEV    LA  \\\n",
       "0       2018     Ryder Jones     Giants     8       3  109.1  112.1  18.3   \n",
       "1       2016  Eric Young Jr.    Yankees     1       1  107.9  107.9  12.5   \n",
       "2       2017     Aaron Blair     Braves     2       1  106.8  106.8  -2.5   \n",
       "3       2017      Erik Kratz    Yankees     2       2  106.0  108.6   5.5   \n",
       "4       2019    Jesus Tinoco    Rockies     6       2  106.0  106.0 -17.0   \n",
       "...      ...             ...        ...   ...     ...    ...    ...   ...   \n",
       "4034    2016      Jumbo Diaz       Reds     1       1   51.0   51.0  -7.7   \n",
       "4035    2018     Austin Voth  Nationals     2       1   45.3   45.3 -31.6   \n",
       "4036    2016  Dean Kiekhefer  Cardinals     1       1   44.1   44.1 -68.9   \n",
       "4037    2017     Drew Storen       Reds     1       1   36.7   36.7  20.7   \n",
       "4038    2018      Rocky Gale    Dodgers     2       1   30.9   30.9  64.6   \n",
       "\n",
       "      Barrels  Barrel%  ...       HR%  BB%  IBB%       SO%  HBP%  SF%  SH%  \\\n",
       "0         2.0     66.7  ...  0.666667  0.0   0.0  1.666667   0.0  0.0  0.0   \n",
       "1         0.0      0.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   \n",
       "2         0.0      0.0  ...  0.000000  1.0   0.0  0.000000   0.0  0.0  0.0   \n",
       "3         1.0     50.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   \n",
       "4         0.0      0.0  ...  0.000000  0.0   0.0  2.000000   0.0  0.0  0.5   \n",
       "...       ...      ...  ...       ...  ...   ...       ...   ...  ...  ...   \n",
       "4034      0.0      0.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   \n",
       "4035      0.0      0.0  ...  0.000000  0.0   0.0  1.000000   0.0  0.0  0.0   \n",
       "4036      0.0      0.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   \n",
       "4037      0.0      0.0  ...  0.000000  0.0   0.0  0.000000   0.0  0.0  0.0   \n",
       "4038      0.0      0.0  ...  0.000000  0.0   0.0  1.000000   0.0  0.0  0.0   \n",
       "\n",
       "      GDP%  SB%  CS%  \n",
       "0      0.0  0.0  0.0  \n",
       "1      0.0  1.0  0.0  \n",
       "2      0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  \n",
       "...    ...  ...  ...  \n",
       "4034   0.0  0.0  0.0  \n",
       "4035   0.0  0.0  0.0  \n",
       "4036   0.0  0.0  0.0  \n",
       "4037   0.0  0.0  0.0  \n",
       "4038   0.0  0.0  0.0  \n",
       "\n",
       "[3561 rows x 50 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>Name</th>\n      <th>Team</th>\n      <th>PA_x</th>\n      <th>Events</th>\n      <th>EV</th>\n      <th>maxEV</th>\n      <th>LA</th>\n      <th>Barrels</th>\n      <th>Barrel%</th>\n      <th>...</th>\n      <th>HR%</th>\n      <th>BB%</th>\n      <th>IBB%</th>\n      <th>SO%</th>\n      <th>HBP%</th>\n      <th>SF%</th>\n      <th>SH%</th>\n      <th>GDP%</th>\n      <th>SB%</th>\n      <th>CS%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018</td>\n      <td>Ryder Jones</td>\n      <td>Giants</td>\n      <td>8</td>\n      <td>3</td>\n      <td>109.1</td>\n      <td>112.1</td>\n      <td>18.3</td>\n      <td>2.0</td>\n      <td>66.7</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016</td>\n      <td>Eric Young Jr.</td>\n      <td>Yankees</td>\n      <td>1</td>\n      <td>1</td>\n      <td>107.9</td>\n      <td>107.9</td>\n      <td>12.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017</td>\n      <td>Aaron Blair</td>\n      <td>Braves</td>\n      <td>2</td>\n      <td>1</td>\n      <td>106.8</td>\n      <td>106.8</td>\n      <td>-2.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017</td>\n      <td>Erik Kratz</td>\n      <td>Yankees</td>\n      <td>2</td>\n      <td>2</td>\n      <td>106.0</td>\n      <td>108.6</td>\n      <td>5.5</td>\n      <td>1.0</td>\n      <td>50.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019</td>\n      <td>Jesus Tinoco</td>\n      <td>Rockies</td>\n      <td>6</td>\n      <td>2</td>\n      <td>106.0</td>\n      <td>106.0</td>\n      <td>-17.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4034</th>\n      <td>2016</td>\n      <td>Jumbo Diaz</td>\n      <td>Reds</td>\n      <td>1</td>\n      <td>1</td>\n      <td>51.0</td>\n      <td>51.0</td>\n      <td>-7.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4035</th>\n      <td>2018</td>\n      <td>Austin Voth</td>\n      <td>Nationals</td>\n      <td>2</td>\n      <td>1</td>\n      <td>45.3</td>\n      <td>45.3</td>\n      <td>-31.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4036</th>\n      <td>2016</td>\n      <td>Dean Kiekhefer</td>\n      <td>Cardinals</td>\n      <td>1</td>\n      <td>1</td>\n      <td>44.1</td>\n      <td>44.1</td>\n      <td>-68.9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4037</th>\n      <td>2017</td>\n      <td>Drew Storen</td>\n      <td>Reds</td>\n      <td>1</td>\n      <td>1</td>\n      <td>36.7</td>\n      <td>36.7</td>\n      <td>20.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4038</th>\n      <td>2018</td>\n      <td>Rocky Gale</td>\n      <td>Dodgers</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30.9</td>\n      <td>30.9</td>\n      <td>64.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3561 rows Ã— 50 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiseason_lines(df, stats, num_seasons):\n",
    "    \"\"\"\n",
    "    Adds columns to rows in a DataFrame for previous seasons; useful for making past seasons features\n",
    "    in ML applicaitons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        A DataFrame that includes multiple seasons of player data in separate rows.\n",
    "        The DataFrame must include a unique 'playerid' column (per Fangraphs) and \n",
    "        the seasons must be labeled 'Season' with int-type data.\n",
    "    stats: list-like\n",
    "        The stats to include from previous seasons.\n",
    "\n",
    "    num_seasons: int\n",
    "        The number of past seasons to include as past stats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A DataFrame with the columns from df and additional columns from stats\n",
    "        labeled with suffixes '_1', '_2'...'_x' for the past season's stats. Players\n",
    "        without stats from the previous season are NaN valued.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        >>> df\n",
    "        playerid | Season | Player | Batting Average\n",
    "        001        2001     Ichiro   .350\n",
    "        001        2002     Ichiro   .321\n",
    "        >>> multiseason_lines(df, ['Batting_Average'], 1)\n",
    "        playerid | Season | Player | Batting Average | Batting Average_1\n",
    "        001        2002     Ichiro   .350              .321\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for n in range(num_seasons):\n",
    "        df1 = df.copy()\n",
    "        df1['Season'] = df1['Season'] + 1 + n ##we set df1 to match df, except that all season numbers are incremented.\n",
    "        out = pd.merge(out, df1[stats], how = 'left', on=['playerid','Season'], #then we can align a previous season with a current one.\n",
    "            suffixes=(\"\",\"_\"+str(1+n)))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Season', 'Name', 'Team', 'PA_x', 'Events', 'EV', 'maxEV', 'LA',\n",
       "       'Barrels', 'Barrel%', 'HardHit', 'HardHit%', 'playerid', 'cosLA',\n",
       "       'sinLA', 'EV_x', 'EV_y', 'Hangtime', 'Distance', 'LA_optimality', 'PA',\n",
       "       'H', '1B', '2B', '3B', 'HR', 'BB', 'IBB', 'SO', 'HBP', 'SF', 'SH',\n",
       "       'GDP', 'SB', 'CS', 'PA%', 'H%', '1B%', '2B%', '3B%', 'HR%', 'BB%',\n",
       "       'IBB%', 'SO%', 'HBP%', 'SF%', 'SH%', 'GDP%', 'SB%', 'CS%', 'PA_x_1',\n",
       "       'Events_1', 'EV_1', 'maxEV_1', 'LA_1', 'Barrels_1', 'Barrel%_1',\n",
       "       'HardHit_1', 'HardHit%_1', 'cosLA_1', 'sinLA_1', 'EV_x_1', 'EV_y_1',\n",
       "       'Hangtime_1', 'Distance_1', 'LA_optimality_1', 'PA_1', 'H_1', '1B_1',\n",
       "       '2B_1', '3B_1', 'HR_1', 'BB_1', 'IBB_1', 'SO_1', 'HBP_1', 'SF_1',\n",
       "       'SH_1', 'GDP_1', 'SB_1', 'CS_1', 'PA%_1', 'H%_1', '1B%_1', '2B%_1',\n",
       "       '3B%_1', 'HR%_1', 'BB%_1', 'IBB%_1', 'SO%_1', 'HBP%_1', 'SF%_1',\n",
       "       'SH%_1', 'GDP%_1', 'SB%_1', 'CS%_1'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "stats = df.select_dtypes(exclude='object').columns\n",
    "df = multiseason_lines(df,stats,1).dropna()\n",
    "event_threshold = 100\n",
    "df = df[(df['Events'] > event_threshold) & (df['Events_1']> event_threshold)]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "correlations = df.corr()\n",
    "df['PA_x'].min()"
   ]
  },
  {
   "source": [
    "## Lots of data cleaning and prep above.\n",
    "\n",
    "Our goal: let's predict subsequent season HR based on previous season stat cast data. We will also compare this to using previous season box score data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['PA_1',\n",
       " 'Events_1',\n",
       " 'EV_1',\n",
       " 'maxEV_1',\n",
       " 'LA_1',\n",
       " 'Barrels_1',\n",
       " 'Barrel%_1',\n",
       " 'HardHit_1',\n",
       " 'HardHit%_1',\n",
       " 'cosLA_1',\n",
       " 'sinLA_1',\n",
       " 'EV_x_1',\n",
       " 'EV_y_1',\n",
       " 'Hangtime_1',\n",
       " 'Distance_1',\n",
       " 'LA_optimality_1']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "statcast_cols\n",
    "statcast_cols_prev = [x + '_1' for x in statcast_cols]\n",
    "statcast_cols_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['HR%']\n",
    "X = df[[ x for x in df.select_dtypes(exclude='object').columns if x.endswith(\"_1\")]]\n",
    "X = X[statcast_cols_prev]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['PA_1', 'Events_1', 'EV_1', 'maxEV_1', 'LA_1', 'Barrels_1', 'Barrel%_1',\n",
       "       'HardHit_1', 'HardHit%_1', 'cosLA_1', 'sinLA_1', 'EV_x_1', 'EV_y_1',\n",
       "       'Hangtime_1', 'Distance_1', 'LA_optimality_1'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing a Baseline\n",
    "\n",
    "Any decent information we get should improve on using a simple linear regression of a previous season's HR total.\n",
    "So, we findout how that scores.\n",
    "\n",
    "...\n",
    "\n",
    "They scored roughly .37 and .40; the test was actually a little higher than the train, which is just a result of the split (random chance.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.24026066677237912\n0.017379435517525698\n0.0005303982062377767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## The most basic model, HR% = HR%_1\n",
    "temp = df[[ x for x in df.select_dtypes(exclude='object').columns if x.endswith(\"_1\")]]\n",
    "print(r2_score(y, temp['HR%_1']))\n",
    "print(mean_absolute_error(temp['HR%_1'], y))\n",
    "print(mean_squared_error(temp['HR%_1'], y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5396376384510931 0.43953426011833696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression().fit(X_train, y_train)\n",
    "linear.fit(X_train,y_train)\n",
    "print(linear.score(X_train, y_train),\n",
    "    linear.score(X_test, y_test))"
   ]
  },
  {
   "source": [
    "### The basic linear regression is good\n",
    "\n",
    "It improves performance over a simple linear regression on past season HR%.LinearRegression\n",
    "\n",
    "Next we look at PCA regressions. \n",
    "\n",
    "### PCA\n",
    "\n",
    "PCA (Principle Component Analysis) is a dimensionality reduction algorithm that produces produces axes capturing as much of the data variance is possible. Each dimesion is independent of the others, i.e. the second dimension captures as much of variance that the first dimension don't capture. Our data dimensionality is small, but the following will show:\n",
    "- The statcast data can be efficiently reduced to about 3 dimensions without loss of accuracy.\n",
    "- With PCA, polynomial features improve training (but not testing) performance, suggesting that there are not important interaction among the variables. (i.e., polynomial features allow us to overfit out training data.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=20)\n",
    "\n",
    "pca_model = Pipeline(\n",
    "    steps=[('scaler', StandardScaler()),\n",
    "        #('poly_features', PolynomialFeatures(degree=2)),\n",
    "        ('pca', PCA(n_components=3)),\n",
    "        ('linear', LinearRegression())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4967397073384652\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.43246683486402193"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "pca_model.fit(X_train, y_train)\n",
    "print(pca_model.score(X_train, y_train))\n",
    "pca_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "poly_no_pca  = Pipeline(\n",
    "    steps=[('scaler', StandardScaler()),\n",
    "        ('poly_features', PolynomialFeatures(degree=2)),\n",
    "        #('pca', PCA(n_components=100)),\n",
    "        ('linear', Ridge(alpha=5))]\n",
    ")\n",
    "pca_ridge = Pipeline(\n",
    "    steps=[('scaler', StandardScaler()),\n",
    "        ('poly_features', PolynomialFeatures(degree=2)),\n",
    "        ('pca', PCA(n_components=15)),\n",
    "        ('linear', Ridge(alpha=.01))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('poly_features',\n",
       "                                              PolynomialFeatures()),\n",
       "                                             ('pca', PCA(n_components=15)),\n",
       "                                             ('linear', Ridge(alpha=0.01))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'linear__alpha': array([  10.        ,   10.04620421,   10.09262191,   10.13925408,\n",
       "         10.1861017 ,   10.23316578,   10.28044732,   10.32794732,\n",
       "         10.37566679,   10.42360674,   10.47176819,...\n",
       "        90,  90,  90,  91,  91,  91,  91,  91,  91,  91,  91,  91,  91,\n",
       "        91,  91,  91,  92,  92,  92,  92,  92,  92,  92,  92,  92,  92,\n",
       "        92,  92,  93,  93,  93,  93,  93,  93,  93,  93,  93,  93,  93,\n",
       "        93,  93,  94,  94,  94,  94,  94,  94,  94,  94,  94,  94,  94,\n",
       "        94,  95,  95,  95,  95,  95,  95,  95,  95,  95,  95,  95,  95,\n",
       "        95,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,\n",
       "        97,  97,  97,  97,  97,  97,  97,  97,  97,  97,  97,  97,  97,\n",
       "        98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  99,\n",
       "        99,  99,  99,  99,  99,  99,  99,  99,  99,  99,  99, 100])})"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {'linear__alpha': np.logspace(1,3,num=1000), 'pca__n_components':np.linspace(20,100,num=1000).astype(int)}\n",
    "grid = RandomizedSearchCV(pca_ridge, param_grid, n_iter=100)\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)[['params','rank_test_score', 'mean_test_score']].sort_values('mean_test_score', ascending=False)\n",
    "results['Components'] = results['params'].apply(lambda x: x['pca__n_components'])\n",
    "results['L2'] = results['params'].apply(lambda x: x['linear__alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               params  rank_test_score  \\\n",
       "67  {'pca__n_components': 27, 'linear__alpha': 73....                1   \n",
       "75  {'pca__n_components': 30, 'linear__alpha': 112...                2   \n",
       "65  {'pca__n_components': 45, 'linear__alpha': 102...                3   \n",
       "62  {'pca__n_components': 70, 'linear__alpha': 125...                4   \n",
       "13  {'pca__n_components': 66, 'linear__alpha': 112...                5   \n",
       "91  {'pca__n_components': 53, 'linear__alpha': 106...                6   \n",
       "7   {'pca__n_components': 85, 'linear__alpha': 147...                7   \n",
       "24  {'pca__n_components': 89, 'linear__alpha': 95....                8   \n",
       "37  {'pca__n_components': 72, 'linear__alpha': 154...                9   \n",
       "35  {'pca__n_components': 67, 'linear__alpha': 157...               10   \n",
       "\n",
       "    mean_test_score  Components          L2  \n",
       "67         0.507715          27   73.598145  \n",
       "75         0.505809          30  112.993394  \n",
       "65         0.505159          45  102.096066  \n",
       "62         0.505105          70  125.631660  \n",
       "13         0.505087          66  112.473718  \n",
       "91         0.504984          53  106.912634  \n",
       "7          0.504918          85  147.628147  \n",
       "24         0.504873          89   95.715215  \n",
       "37         0.504815          72  154.592774  \n",
       "35         0.504767          67  157.469771  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>params</th>\n      <th>rank_test_score</th>\n      <th>mean_test_score</th>\n      <th>Components</th>\n      <th>L2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>67</th>\n      <td>{'pca__n_components': 27, 'linear__alpha': 73....</td>\n      <td>1</td>\n      <td>0.507715</td>\n      <td>27</td>\n      <td>73.598145</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>{'pca__n_components': 30, 'linear__alpha': 112...</td>\n      <td>2</td>\n      <td>0.505809</td>\n      <td>30</td>\n      <td>112.993394</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>{'pca__n_components': 45, 'linear__alpha': 102...</td>\n      <td>3</td>\n      <td>0.505159</td>\n      <td>45</td>\n      <td>102.096066</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>{'pca__n_components': 70, 'linear__alpha': 125...</td>\n      <td>4</td>\n      <td>0.505105</td>\n      <td>70</td>\n      <td>125.631660</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>{'pca__n_components': 66, 'linear__alpha': 112...</td>\n      <td>5</td>\n      <td>0.505087</td>\n      <td>66</td>\n      <td>112.473718</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>{'pca__n_components': 53, 'linear__alpha': 106...</td>\n      <td>6</td>\n      <td>0.504984</td>\n      <td>53</td>\n      <td>106.912634</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>{'pca__n_components': 85, 'linear__alpha': 147...</td>\n      <td>7</td>\n      <td>0.504918</td>\n      <td>85</td>\n      <td>147.628147</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>{'pca__n_components': 89, 'linear__alpha': 95....</td>\n      <td>8</td>\n      <td>0.504873</td>\n      <td>89</td>\n      <td>95.715215</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>{'pca__n_components': 72, 'linear__alpha': 154...</td>\n      <td>9</td>\n      <td>0.504815</td>\n      <td>72</td>\n      <td>154.592774</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>{'pca__n_components': 67, 'linear__alpha': 157...</td>\n      <td>10</td>\n      <td>0.504767</td>\n      <td>67</td>\n      <td>157.469771</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('poly_features',\n",
       "                                              PolynomialFeatures()),\n",
       "                                             ('pca', PCA(n_components=15)),\n",
       "                                             ('linear', Ridge(alpha=0.01))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'linear__alpha': array([ 10.        ,  10.30697715,  10.62337779,  10.94949121,\n",
       "        11.28561557,  11.63205818,  11.98913578,  12.35717485,\n",
       "        12.73651188,  13.12749369,  13.53047775...\n",
       "        88.19714876,  90.90459967,  93.69516314,  96.57139053,\n",
       "        99.53591153, 102.59143655, 105.7407592 , 108.98675886,\n",
       "       112.3324033 , 115.78075137, 119.33495585, 122.99826628,\n",
       "       126.77403197, 130.66570504, 134.67684358, 138.81111491,\n",
       "       143.07229892, 147.46429154, 151.9911083 , 156.65688798,\n",
       "       161.46589644, 166.42253047, 171.53132184, 176.79694142,\n",
       "       182.2242035 , 187.81807012, 193.58365566, 199.5262315 ]),\n",
       "                                        'pca__n_components': [25, 30, 35]})"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "grid.param_distributions['linear__alpha'] = np.logspace(1,2.3,num=100)\n",
    "grid.param_distributions['pca__n_components'] = [25,30,35]\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)[['params','rank_test_score', 'mean_test_score']].sort_values('mean_test_score', ascending=False)\n",
    "results['Components'] = results['params'].apply(lambda x: x['pca__n_components'])\n",
    "results['L2'] = results['params'].apply(lambda x: x['linear__alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               params  rank_test_score  \\\n",
       "99  {'pca__n_components': 25, 'linear__alpha': 52....                1   \n",
       "11  {'pca__n_components': 25, 'linear__alpha': 48....                2   \n",
       "24  {'pca__n_components': 25, 'linear__alpha': 45....                3   \n",
       "1   {'pca__n_components': 25, 'linear__alpha': 65....                4   \n",
       "59  {'pca__n_components': 25, 'linear__alpha': 73....                5   \n",
       "\n",
       "    mean_test_score  Components         L2  \n",
       "99         0.510480          25  52.749971  \n",
       "11         0.510470          25  48.175728  \n",
       "24         0.510439          25  45.348785  \n",
       "1          0.510433          25  65.184061  \n",
       "59         0.510337          25  73.564225  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>params</th>\n      <th>rank_test_score</th>\n      <th>mean_test_score</th>\n      <th>Components</th>\n      <th>L2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>99</th>\n      <td>{'pca__n_components': 25, 'linear__alpha': 52....</td>\n      <td>1</td>\n      <td>0.510480</td>\n      <td>25</td>\n      <td>52.749971</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>{'pca__n_components': 25, 'linear__alpha': 48....</td>\n      <td>2</td>\n      <td>0.510470</td>\n      <td>25</td>\n      <td>48.175728</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>{'pca__n_components': 25, 'linear__alpha': 45....</td>\n      <td>3</td>\n      <td>0.510439</td>\n      <td>25</td>\n      <td>45.348785</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{'pca__n_components': 25, 'linear__alpha': 65....</td>\n      <td>4</td>\n      <td>0.510433</td>\n      <td>25</td>\n      <td>65.184061</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>{'pca__n_components': 25, 'linear__alpha': 73....</td>\n      <td>5</td>\n      <td>0.510337</td>\n      <td>25</td>\n      <td>73.564225</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.42169339247547544"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "pca_ridge = grid.best_estimator_\n",
    "pca_ridge.fit(X_train, y_train)\n",
    "pca_ridge.score(X_train, y_train)\n",
    "pca_ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "plsr = PLSRegression(n_components=2)\n",
    "\n",
    "plsr = Pipeline(\n",
    "    steps=[('scaler', StandardScaler()),\n",
    "        ('poly_features', PolynomialFeatures(degree=2)),\n",
    "        ('plsr', plsr)\n",
    "    ]\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('poly_features', PolynomialFeatures()),\n",
       "                                       ('plsr', PLSRegression())]),\n",
       "             param_grid={'plsr__n_components': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19])})"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "plsr_grid = GridSearchCV(plsr, param_grid={'plsr__n_components' :np.arange(2,20)})\n",
    "plsr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsr = plsr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('poly_features', PolynomialFeatures()),\n",
       "                ('plsr', PLSRegression(n_components=10))])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "plsr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.415183689739943"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "plsr.score(X_train, y_train)\n",
    "plsr.score(X_test, y_test)\n",
    "#pearsonr(plsr.predict(X_test), y_test)\n"
   ]
  },
  {
   "source": [
    "## Summary\n",
    "\n",
    "There's no distinct advantage to using PCA regression or PLS to derive insights from StatCast data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=1000,\n",
    "        learning_rate_init=.0003,\n",
    "        solver='lbfgs',\n",
    "        activation='logistic',\n",
    "        hidden_layer_sizes=80,\n",
    "        random_state=1,\n",
    "        alpha=.01\n",
    "        )\n",
    "mlp = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                ('mlp', mlp)])\n",
    "\n",
    "param_dist = {'mlp__learning_rate_init': [.0003],      \n",
    "                #'mlp__activation': ['logistic'],\n",
    "                #'mlp__solver': ['lbfgs', 'adam'],\n",
    "                #'mlp__hidden_layer_sizes': [80, (80, 40)],\n",
    "                'mlp__alpha': np.logspace(-5,1)}\n",
    "mlp_search = RandomizedSearchCV(mlp, return_train_score=True, param_distributions=param_dist, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('mlp',\n",
       "                                              MLPRegressor(activation='logistic',\n",
       "                                                           alpha=0.01,\n",
       "                                                           hidden_layer_sizes=80,\n",
       "                                                           learning_rate_init=0.0003,\n",
       "                                                           max_iter=1000,\n",
       "                                                           random_state=1,\n",
       "                                                           solver='lbfgs'))]),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'mlp__alpha': array([1.00000000e-05, 1.32571137e-05, 1.75751062e-05, 2.32995181e-05,\n",
       "       3.08884360e-05, 4....\n",
       "       2.68269580e-02, 3.55648031e-02, 4.71486636e-02, 6.25055193e-02,\n",
       "       8.28642773e-02, 1.09854114e-01, 1.45634848e-01, 1.93069773e-01,\n",
       "       2.55954792e-01, 3.39322177e-01, 4.49843267e-01, 5.96362332e-01,\n",
       "       7.90604321e-01, 1.04811313e+00, 1.38949549e+00, 1.84206997e+00,\n",
       "       2.44205309e+00, 3.23745754e+00, 4.29193426e+00, 5.68986603e+00,\n",
       "       7.54312006e+00, 1.00000000e+01]),\n",
       "                                        'mlp__learning_rate_init': [0.0003]},\n",
       "                   return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "mlp_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4        0.029276      0.000488         0.002021        0.000022   \n",
       "8        0.030315      0.002405         0.002370        0.000498   \n",
       "14       0.028495      0.001021         0.002793        0.000754   \n",
       "13       0.029297      0.000489         0.002007        0.000028   \n",
       "18       0.032162      0.000385         0.002591        0.000490   \n",
       "2        0.029294      0.000786         0.002418        0.000493   \n",
       "9        0.029676      0.000398         0.002001        0.000010   \n",
       "7        0.028889      0.000649         0.002799        0.000739   \n",
       "0        0.033190      0.005644         0.002404        0.000482   \n",
       "12       0.028684      0.000402         0.002005        0.000028   \n",
       "16       0.028283      0.000498         0.002592        0.000502   \n",
       "10       0.030094      0.001713         0.002211        0.000388   \n",
       "17       0.077867      0.019581         0.002401        0.000482   \n",
       "3        0.058217      0.010798         0.002227        0.000397   \n",
       "1        0.082553      0.012416         0.002601        0.000477   \n",
       "15       0.056222      0.006253         0.002589        0.000469   \n",
       "5        0.050638      0.009051         0.002023        0.000021   \n",
       "6        0.042482      0.000811         0.002804        0.000382   \n",
       "11       0.043246      0.001503         0.002007        0.000016   \n",
       "19       0.040032      0.001483         0.002393        0.000489   \n",
       "\n",
       "   param_mlp__learning_rate_init param_mlp__alpha  \\\n",
       "4                         0.0003       0.00868511   \n",
       "8                         0.0003        0.0152642   \n",
       "14                        0.0003        0.0202359   \n",
       "13                        0.0003      0.000517947   \n",
       "18                        0.0003      0.000390694   \n",
       "2                         0.0003        0.0002223   \n",
       "9                         0.0003      0.000167683   \n",
       "7                         0.0003      0.000126486   \n",
       "0                         0.0003      5.42868e-05   \n",
       "12                        0.0003      4.09492e-05   \n",
       "16                        0.0003      1.32571e-05   \n",
       "10                        0.0003         0.026827   \n",
       "17                        0.0003        0.0625055   \n",
       "3                         0.0003         0.449843   \n",
       "1                         0.0003         0.255955   \n",
       "15                        0.0003         0.109854   \n",
       "5                         0.0003          2.44205   \n",
       "6                         0.0003          4.29193   \n",
       "11                        0.0003          5.68987   \n",
       "19                        0.0003               10   \n",
       "\n",
       "                                               params  mean_test_score  \\\n",
       "4   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508591   \n",
       "8   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508590   \n",
       "14  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508541   \n",
       "13  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508529   \n",
       "18  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508528   \n",
       "2   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508526   \n",
       "9   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508526   \n",
       "7   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508525   \n",
       "0   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508524   \n",
       "12  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508524   \n",
       "16  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508524   \n",
       "10  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.508323   \n",
       "17  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.500985   \n",
       "3   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.491933   \n",
       "1   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.487785   \n",
       "15  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.486516   \n",
       "5   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.367279   \n",
       "6   {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.230427   \n",
       "11  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.186754   \n",
       "19  {'mlp__learning_rate_init': 0.0003, 'mlp__alph...         0.029145   \n",
       "\n",
       "    std_test_score  rank_test_score  mean_train_score  std_train_score  \n",
       "4         0.087366                1          0.530703         0.022834  \n",
       "8         0.087535                2          0.530640         0.022841  \n",
       "14        0.087739                3          0.530536         0.022845  \n",
       "13        0.087313                4          0.530700         0.022826  \n",
       "18        0.087314                5          0.530699         0.022826  \n",
       "2         0.087315                6          0.530698         0.022826  \n",
       "9         0.087315                7          0.530698         0.022826  \n",
       "7         0.087315                8          0.530697         0.022826  \n",
       "0         0.087316                9          0.530697         0.022826  \n",
       "12        0.087316               10          0.530697         0.022826  \n",
       "16        0.087316               11          0.530697         0.022826  \n",
       "10        0.086148               12          0.532136         0.023210  \n",
       "17        0.076679               13          0.522477         0.024913  \n",
       "3         0.062140               14          0.510811         0.027481  \n",
       "1         0.077095               15          0.501184         0.027981  \n",
       "15        0.075573               16          0.505845         0.022066  \n",
       "5         0.061280               17          0.376489         0.023781  \n",
       "6         0.051477               18          0.234874         0.040260  \n",
       "11        0.033783               19          0.186562         0.014008  \n",
       "19        0.012062               20          0.030556         0.014163  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_mlp__learning_rate_init</th>\n      <th>param_mlp__alpha</th>\n      <th>params</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>0.029276</td>\n      <td>0.000488</td>\n      <td>0.002021</td>\n      <td>0.000022</td>\n      <td>0.0003</td>\n      <td>0.00868511</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508591</td>\n      <td>0.087366</td>\n      <td>1</td>\n      <td>0.530703</td>\n      <td>0.022834</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.030315</td>\n      <td>0.002405</td>\n      <td>0.002370</td>\n      <td>0.000498</td>\n      <td>0.0003</td>\n      <td>0.0152642</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508590</td>\n      <td>0.087535</td>\n      <td>2</td>\n      <td>0.530640</td>\n      <td>0.022841</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.028495</td>\n      <td>0.001021</td>\n      <td>0.002793</td>\n      <td>0.000754</td>\n      <td>0.0003</td>\n      <td>0.0202359</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508541</td>\n      <td>0.087739</td>\n      <td>3</td>\n      <td>0.530536</td>\n      <td>0.022845</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.029297</td>\n      <td>0.000489</td>\n      <td>0.002007</td>\n      <td>0.000028</td>\n      <td>0.0003</td>\n      <td>0.000517947</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508529</td>\n      <td>0.087313</td>\n      <td>4</td>\n      <td>0.530700</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.032162</td>\n      <td>0.000385</td>\n      <td>0.002591</td>\n      <td>0.000490</td>\n      <td>0.0003</td>\n      <td>0.000390694</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508528</td>\n      <td>0.087314</td>\n      <td>5</td>\n      <td>0.530699</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.029294</td>\n      <td>0.000786</td>\n      <td>0.002418</td>\n      <td>0.000493</td>\n      <td>0.0003</td>\n      <td>0.0002223</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508526</td>\n      <td>0.087315</td>\n      <td>6</td>\n      <td>0.530698</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.029676</td>\n      <td>0.000398</td>\n      <td>0.002001</td>\n      <td>0.000010</td>\n      <td>0.0003</td>\n      <td>0.000167683</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508526</td>\n      <td>0.087315</td>\n      <td>7</td>\n      <td>0.530698</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.028889</td>\n      <td>0.000649</td>\n      <td>0.002799</td>\n      <td>0.000739</td>\n      <td>0.0003</td>\n      <td>0.000126486</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508525</td>\n      <td>0.087315</td>\n      <td>8</td>\n      <td>0.530697</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.033190</td>\n      <td>0.005644</td>\n      <td>0.002404</td>\n      <td>0.000482</td>\n      <td>0.0003</td>\n      <td>5.42868e-05</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508524</td>\n      <td>0.087316</td>\n      <td>9</td>\n      <td>0.530697</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.028684</td>\n      <td>0.000402</td>\n      <td>0.002005</td>\n      <td>0.000028</td>\n      <td>0.0003</td>\n      <td>4.09492e-05</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508524</td>\n      <td>0.087316</td>\n      <td>10</td>\n      <td>0.530697</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.028283</td>\n      <td>0.000498</td>\n      <td>0.002592</td>\n      <td>0.000502</td>\n      <td>0.0003</td>\n      <td>1.32571e-05</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508524</td>\n      <td>0.087316</td>\n      <td>11</td>\n      <td>0.530697</td>\n      <td>0.022826</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.030094</td>\n      <td>0.001713</td>\n      <td>0.002211</td>\n      <td>0.000388</td>\n      <td>0.0003</td>\n      <td>0.026827</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.508323</td>\n      <td>0.086148</td>\n      <td>12</td>\n      <td>0.532136</td>\n      <td>0.023210</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.077867</td>\n      <td>0.019581</td>\n      <td>0.002401</td>\n      <td>0.000482</td>\n      <td>0.0003</td>\n      <td>0.0625055</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.500985</td>\n      <td>0.076679</td>\n      <td>13</td>\n      <td>0.522477</td>\n      <td>0.024913</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.058217</td>\n      <td>0.010798</td>\n      <td>0.002227</td>\n      <td>0.000397</td>\n      <td>0.0003</td>\n      <td>0.449843</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.491933</td>\n      <td>0.062140</td>\n      <td>14</td>\n      <td>0.510811</td>\n      <td>0.027481</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.082553</td>\n      <td>0.012416</td>\n      <td>0.002601</td>\n      <td>0.000477</td>\n      <td>0.0003</td>\n      <td>0.255955</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.487785</td>\n      <td>0.077095</td>\n      <td>15</td>\n      <td>0.501184</td>\n      <td>0.027981</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.056222</td>\n      <td>0.006253</td>\n      <td>0.002589</td>\n      <td>0.000469</td>\n      <td>0.0003</td>\n      <td>0.109854</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.486516</td>\n      <td>0.075573</td>\n      <td>16</td>\n      <td>0.505845</td>\n      <td>0.022066</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.050638</td>\n      <td>0.009051</td>\n      <td>0.002023</td>\n      <td>0.000021</td>\n      <td>0.0003</td>\n      <td>2.44205</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.367279</td>\n      <td>0.061280</td>\n      <td>17</td>\n      <td>0.376489</td>\n      <td>0.023781</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.042482</td>\n      <td>0.000811</td>\n      <td>0.002804</td>\n      <td>0.000382</td>\n      <td>0.0003</td>\n      <td>4.29193</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.230427</td>\n      <td>0.051477</td>\n      <td>18</td>\n      <td>0.234874</td>\n      <td>0.040260</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.043246</td>\n      <td>0.001503</td>\n      <td>0.002007</td>\n      <td>0.000016</td>\n      <td>0.0003</td>\n      <td>5.68987</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.186754</td>\n      <td>0.033783</td>\n      <td>19</td>\n      <td>0.186562</td>\n      <td>0.014008</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.040032</td>\n      <td>0.001483</td>\n      <td>0.002393</td>\n      <td>0.000489</td>\n      <td>0.0003</td>\n      <td>10</td>\n      <td>{'mlp__learning_rate_init': 0.0003, 'mlp__alph...</td>\n      <td>0.029145</td>\n      <td>0.012062</td>\n      <td>20</td>\n      <td>0.030556</td>\n      <td>0.014163</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "results = pd.DataFrame(mlp_search.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "cols = [col for col in results if col[:5]!='split']\n",
    "results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'scaler': StandardScaler(), 'mlp': MLPRegressor(activation='logistic', alpha=0.01, hidden_layer_sizes=80,\n             learning_rate_init=0.0003, max_iter=1000, random_state=1,\n             solver='lbfgs')}\n0.5288422036139779\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4238314216524087"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "print(mlp.named_steps)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_train, y_train))\n",
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "source": [
    "### MLP does no better\n",
    "\n",
    "At least simple MLP Regression doesn't improve on the linear regression model above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marcel import MarcelForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_data = pd.read_csv('../data/pitchers_since_1947.csv')\n",
    "pitcher_data = pitcher_data[pitcher_data['Season'] >= 2016]\n",
    "marcel = MarcelForecaster(pitcher_data, standard, as_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "141.0"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "X_train['PA_1'].min()"
   ]
  },
  {
   "source": [
    "## Let's compare with Marcel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = standard[(standard['PA'] > 100) & (standard['Season'] == 2018)]['playerid'].unique()\n",
    "ids in ids\n",
    "marcel.hitters = marcel.hitters[marcel.hitters['playerid'].isin(ids)]\n",
    "results = marcel.project_hitters(2019, apply_age=False).drop('playerid',axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   playerid  Season             Name       Team     PA           H         1B  \\\n",
       "0       393    2019  Victor Martinez     Tigers  497.5  118.352205  84.269869   \n",
       "1       639    2019    Adrian Beltre    Rangers  479.4  122.729532  78.970170   \n",
       "2       785    2019     Todd Frazier  White Sox  493.6   94.970929  54.140773   \n",
       "3      1159    2019    Andrew Romine     Tigers  300.3   64.370961  45.972688   \n",
       "4      1177    2019    Albert Pujols     Angels  512.6  118.538265  80.677287   \n",
       "\n",
       "          2B        3B         HR         BB       IBB          SO       HBP  \\\n",
       "0  19.739272  0.383359  13.959705  37.888979  4.252455   69.423185  3.668681   \n",
       "1  23.753715  1.191349  18.814299  38.771245  2.880280   77.141648  5.134518   \n",
       "2  17.833912  0.604028  22.392217  54.356929  1.419104  113.286493  7.515606   \n",
       "3  11.643730  1.927432   4.827111  20.830144  0.521647   65.055638  3.523968   \n",
       "4  17.541915  0.352812  19.966251  33.683374  3.726265   73.628770  2.252311   \n",
       "\n",
       "         SF        SH        GDP        SB        CS       OBP  \n",
       "0  4.069768  0.190575  16.087523  1.173136  0.443662  0.321427  \n",
       "1  5.466013  0.189581   9.959990  1.977005  0.645063  0.347591  \n",
       "2  5.569606  0.364720   9.236290  7.717252  3.368932  0.317754  \n",
       "3  1.249433  3.130148   6.031450  5.496097  1.769875  0.295455  \n",
       "4  3.558135  0.175389  16.313351  2.890606  0.408310  0.301354  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>playerid</th>\n      <th>Season</th>\n      <th>Name</th>\n      <th>Team</th>\n      <th>PA</th>\n      <th>H</th>\n      <th>1B</th>\n      <th>2B</th>\n      <th>3B</th>\n      <th>HR</th>\n      <th>BB</th>\n      <th>IBB</th>\n      <th>SO</th>\n      <th>HBP</th>\n      <th>SF</th>\n      <th>SH</th>\n      <th>GDP</th>\n      <th>SB</th>\n      <th>CS</th>\n      <th>OBP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>393</td>\n      <td>2019</td>\n      <td>Victor Martinez</td>\n      <td>Tigers</td>\n      <td>497.5</td>\n      <td>118.352205</td>\n      <td>84.269869</td>\n      <td>19.739272</td>\n      <td>0.383359</td>\n      <td>13.959705</td>\n      <td>37.888979</td>\n      <td>4.252455</td>\n      <td>69.423185</td>\n      <td>3.668681</td>\n      <td>4.069768</td>\n      <td>0.190575</td>\n      <td>16.087523</td>\n      <td>1.173136</td>\n      <td>0.443662</td>\n      <td>0.321427</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>639</td>\n      <td>2019</td>\n      <td>Adrian Beltre</td>\n      <td>Rangers</td>\n      <td>479.4</td>\n      <td>122.729532</td>\n      <td>78.970170</td>\n      <td>23.753715</td>\n      <td>1.191349</td>\n      <td>18.814299</td>\n      <td>38.771245</td>\n      <td>2.880280</td>\n      <td>77.141648</td>\n      <td>5.134518</td>\n      <td>5.466013</td>\n      <td>0.189581</td>\n      <td>9.959990</td>\n      <td>1.977005</td>\n      <td>0.645063</td>\n      <td>0.347591</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>785</td>\n      <td>2019</td>\n      <td>Todd Frazier</td>\n      <td>White Sox</td>\n      <td>493.6</td>\n      <td>94.970929</td>\n      <td>54.140773</td>\n      <td>17.833912</td>\n      <td>0.604028</td>\n      <td>22.392217</td>\n      <td>54.356929</td>\n      <td>1.419104</td>\n      <td>113.286493</td>\n      <td>7.515606</td>\n      <td>5.569606</td>\n      <td>0.364720</td>\n      <td>9.236290</td>\n      <td>7.717252</td>\n      <td>3.368932</td>\n      <td>0.317754</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1159</td>\n      <td>2019</td>\n      <td>Andrew Romine</td>\n      <td>Tigers</td>\n      <td>300.3</td>\n      <td>64.370961</td>\n      <td>45.972688</td>\n      <td>11.643730</td>\n      <td>1.927432</td>\n      <td>4.827111</td>\n      <td>20.830144</td>\n      <td>0.521647</td>\n      <td>65.055638</td>\n      <td>3.523968</td>\n      <td>1.249433</td>\n      <td>3.130148</td>\n      <td>6.031450</td>\n      <td>5.496097</td>\n      <td>1.769875</td>\n      <td>0.295455</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1177</td>\n      <td>2019</td>\n      <td>Albert Pujols</td>\n      <td>Angels</td>\n      <td>512.6</td>\n      <td>118.538265</td>\n      <td>80.677287</td>\n      <td>17.541915</td>\n      <td>0.352812</td>\n      <td>19.966251</td>\n      <td>33.683374</td>\n      <td>3.726265</td>\n      <td>73.628770</td>\n      <td>2.252311</td>\n      <td>3.558135</td>\n      <td>0.175389</td>\n      <td>16.313351</td>\n      <td>2.890606</td>\n      <td>0.408310</td>\n      <td>0.301354</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.297559582377932"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "h2019 = marcel.hitters[marcel.hitters['Season'] == 2019].sort_values('playerid')\n",
    "results\n",
    "results['HR%'] = results['HR']/results['PA']\n",
    "x_pred= results[results['playerid'].isin(h2019['playerid'])]\n",
    "h2019['HR%'] = h2019['HR']/h2019['PA']\n",
    "r2_score(h2019['HR%'], x_pred['HR%'])"
   ]
  },
  {
   "source": [
    "## The models were stronger than Marcel\n",
    "\n",
    "I need to double check that. It looks too good to be true."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}