{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbaseconda1fad7997e8f94ea293ffe5b4ecbfb0c4",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the necessities\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Unnamed: 0  index pitch_type   game_date  release_speed  \\\n",
       "0                0      0        NaN  2019-03-21            NaN   \n",
       "1                1      1        NaN  2019-03-21            NaN   \n",
       "5                5      5        NaN  2019-03-21            NaN   \n",
       "7                7      7        NaN  2019-03-21            NaN   \n",
       "12              12     12        NaN  2019-03-21            NaN   \n",
       "...            ...    ...        ...         ...            ...   \n",
       "735427        2954   2954         SL  2019-10-01           87.7   \n",
       "735428        2955   2955         FF  2019-10-01           98.1   \n",
       "735432        2959   2959         FC  2019-10-01           93.0   \n",
       "735438        2965   2965         FF  2019-10-01           98.3   \n",
       "735439        2966   2966         FF  2019-10-01           99.2   \n",
       "\n",
       "        release_pos_x  release_pos_z        player_name    batter   pitcher  \\\n",
       "0                 NaN            NaN  Hunter Strickland  595777.0  519326.0   \n",
       "1                 NaN            NaN  Hunter Strickland  460026.0  519326.0   \n",
       "5                 NaN            NaN  Hunter Strickland  640461.0  519326.0   \n",
       "7                 NaN            NaN    Fernando Rodney  553882.0  407845.0   \n",
       "12                NaN            NaN    Fernando Rodney  570267.0  407845.0   \n",
       "...               ...            ...                ...       ...       ...   \n",
       "735427      -3.558429       4.960761       Max Scherzer  460075.0  453286.0   \n",
       "735428      -3.275855       5.123839       Max Scherzer  669374.0  453286.0   \n",
       "735432      -3.419024       5.001775       Max Scherzer  519058.0  453286.0   \n",
       "735438      -3.325589       5.119030       Max Scherzer  518735.0  453286.0   \n",
       "735439      -3.377274       5.028776       Max Scherzer  663757.0  453286.0   \n",
       "\n",
       "        ... home_score away_score  bat_score  fld_score  post_away_score  \\\n",
       "0       ...        4.0        5.0        4.0        5.0              5.0   \n",
       "1       ...        4.0        5.0        4.0        5.0              5.0   \n",
       "5       ...        4.0        5.0        4.0        5.0              5.0   \n",
       "7       ...        4.0        5.0        5.0        4.0              5.0   \n",
       "12      ...        4.0        4.0        4.0        4.0              4.0   \n",
       "...     ...        ...        ...        ...        ...              ...   \n",
       "735427  ...        0.0        2.0        2.0        0.0              2.0   \n",
       "735428  ...        0.0        2.0        2.0        0.0              2.0   \n",
       "735432  ...        0.0        2.0        2.0        0.0              2.0   \n",
       "735438  ...        0.0        0.0        0.0        0.0              0.0   \n",
       "735439  ...        0.0        0.0        0.0        0.0              0.0   \n",
       "\n",
       "        post_home_score  post_bat_score post_fld_score if_fielding_alignment  \\\n",
       "0                   4.0             4.0            5.0                   NaN   \n",
       "1                   4.0             4.0            5.0                   NaN   \n",
       "5                   4.0             4.0            5.0                   NaN   \n",
       "7                   4.0             5.0            4.0                   NaN   \n",
       "12                  4.0             4.0            4.0                   NaN   \n",
       "...                 ...             ...            ...                   ...   \n",
       "735427              0.0             2.0            0.0              Standard   \n",
       "735428              0.0             2.0            0.0             Strategic   \n",
       "735432              0.0             2.0            0.0              Standard   \n",
       "735438              0.0             0.0            0.0         Infield shift   \n",
       "735439              0.0             0.0            0.0         Infield shift   \n",
       "\n",
       "       of_fielding_alignment  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "5                        NaN  \n",
       "7                        NaN  \n",
       "12                       NaN  \n",
       "...                      ...  \n",
       "735427              Standard  \n",
       "735428              Standard  \n",
       "735432              Standard  \n",
       "735438              Standard  \n",
       "735439              Standard  \n",
       "\n",
       "[186824 rows x 91 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>index</th>\n      <th>pitch_type</th>\n      <th>game_date</th>\n      <th>release_speed</th>\n      <th>release_pos_x</th>\n      <th>release_pos_z</th>\n      <th>player_name</th>\n      <th>batter</th>\n      <th>pitcher</th>\n      <th>...</th>\n      <th>home_score</th>\n      <th>away_score</th>\n      <th>bat_score</th>\n      <th>fld_score</th>\n      <th>post_away_score</th>\n      <th>post_home_score</th>\n      <th>post_bat_score</th>\n      <th>post_fld_score</th>\n      <th>if_fielding_alignment</th>\n      <th>of_fielding_alignment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>2019-03-21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Hunter Strickland</td>\n      <td>595777.0</td>\n      <td>519326.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>2019-03-21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Hunter Strickland</td>\n      <td>460026.0</td>\n      <td>519326.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>2019-03-21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Hunter Strickland</td>\n      <td>640461.0</td>\n      <td>519326.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>2019-03-21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fernando Rodney</td>\n      <td>553882.0</td>\n      <td>407845.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>2019-03-21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fernando Rodney</td>\n      <td>570267.0</td>\n      <td>407845.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>735427</th>\n      <td>2954</td>\n      <td>2954</td>\n      <td>SL</td>\n      <td>2019-10-01</td>\n      <td>87.7</td>\n      <td>-3.558429</td>\n      <td>4.960761</td>\n      <td>Max Scherzer</td>\n      <td>460075.0</td>\n      <td>453286.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>Standard</td>\n      <td>Standard</td>\n    </tr>\n    <tr>\n      <th>735428</th>\n      <td>2955</td>\n      <td>2955</td>\n      <td>FF</td>\n      <td>2019-10-01</td>\n      <td>98.1</td>\n      <td>-3.275855</td>\n      <td>5.123839</td>\n      <td>Max Scherzer</td>\n      <td>669374.0</td>\n      <td>453286.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>Strategic</td>\n      <td>Standard</td>\n    </tr>\n    <tr>\n      <th>735432</th>\n      <td>2959</td>\n      <td>2959</td>\n      <td>FC</td>\n      <td>2019-10-01</td>\n      <td>93.0</td>\n      <td>-3.419024</td>\n      <td>5.001775</td>\n      <td>Max Scherzer</td>\n      <td>519058.0</td>\n      <td>453286.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>Standard</td>\n      <td>Standard</td>\n    </tr>\n    <tr>\n      <th>735438</th>\n      <td>2965</td>\n      <td>2965</td>\n      <td>FF</td>\n      <td>2019-10-01</td>\n      <td>98.3</td>\n      <td>-3.325589</td>\n      <td>5.119030</td>\n      <td>Max Scherzer</td>\n      <td>518735.0</td>\n      <td>453286.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Infield shift</td>\n      <td>Standard</td>\n    </tr>\n    <tr>\n      <th>735439</th>\n      <td>2966</td>\n      <td>2966</td>\n      <td>FF</td>\n      <td>2019-10-01</td>\n      <td>99.2</td>\n      <td>-3.377274</td>\n      <td>5.028776</td>\n      <td>Max Scherzer</td>\n      <td>663757.0</td>\n      <td>453286.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Infield shift</td>\n      <td>Standard</td>\n    </tr>\n  </tbody>\n</table>\n<p>186824 rows × 91 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "## This code occasionally crashes my old, slow computer. \n",
    "\n",
    "df = pd.read_csv('data/statcast_dumps/statcast2019.csv').dropna(subset=['events'])\n",
    "df\n",
    "#all_data = df.copy()\n",
    "#df = df.sample(n=50_000)\n",
    "\n",
    "##So I'm creating a pickle to replace it.abs\n",
    "\n",
    "#pickle_out = open('pickles/hr_clf_data_subset.pickle','wb')\n",
    "#pickle.dump(df, pickle_out)\n",
    "\n",
    "##And now I can load that data with\n",
    "\n",
    "#pickle_in = open('pickles/hr_clf_data_subset.pickle','rb')\n",
    "#df = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'Unnamed: 0'',\n'index'',\n'pitch_type'',\n'game_date'',\n'release_speed'',\n'release_pos_x'',\n'release_pos_z'',\n'player_name'',\n'batter'',\n'pitcher'',\n'events'',\n'description'',\n'spin_dir'',\n'spin_rate_deprecated'',\n'break_angle_deprecated'',\n'break_length_deprecated'',\n'zone'',\n'des'',\n'game_type'',\n'stand'',\n'p_throws'',\n'home_team'',\n'away_team'',\n'type'',\n'hit_location'',\n'bb_type'',\n'balls'',\n'strikes'',\n'game_year'',\n'pfx_x'',\n'pfx_z'',\n'plate_x'',\n'plate_z'',\n'on_3b'',\n'on_2b'',\n'on_1b'',\n'outs_when_up'',\n'inning'',\n'inning_topbot'',\n'hc_x'',\n'hc_y'',\n'tfs_deprecated'',\n'tfs_zulu_deprecated'',\n'fielder_2'',\n'umpire'',\n'sv_id'',\n'vx0'',\n'vy0'',\n'vz0'',\n'ax'',\n'ay'',\n'az'',\n'sz_top'',\n'sz_bot'',\n'hit_distance_sc'',\n'launch_speed'',\n'launch_angle'',\n'effective_speed'',\n'release_spin_rate'',\n'release_extension'',\n'game_pk'',\n'pitcher.1'',\n'fielder_2.1'',\n'fielder_3'',\n'fielder_4'',\n'fielder_5'',\n'fielder_6'',\n'fielder_7'',\n'fielder_8'',\n'fielder_9'',\n'release_pos_y'',\n'estimated_ba_using_speedangle'',\n'estimated_woba_using_speedangle'',\n'woba_value'',\n'woba_denom'',\n'babip_value'',\n'iso_value'',\n'launch_speed_angle'',\n'at_bat_number'',\n'pitch_number'',\n'pitch_name'',\n'home_score'',\n'away_score'',\n'bat_score'',\n'fld_score'',\n'post_away_score'',\n'post_home_score'',\n'post_bat_score'',\n'post_fld_score'',\n'if_fielding_alignment'',\n'of_fielding_alignment'',\n"
     ]
    }
   ],
   "source": [
    "#print(df['des'].unique())\n",
    "for i in df.columns:\n",
    "    print(\"'{}'',\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "318774    Ian Desmond hits an inside-the-park home run (...\n",
       "289952    Hunter Pence hits an inside-the-park home run ...\n",
       "Name: des, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "#Checking for in the park HR.\n",
    "\n",
    "df[df['events'] == 'home_run'].dropna(subset=['hit_location'])['des']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a list of batted ball data. I'm also including batter handedness and home team (which is a proxy for the park.)\n",
    "batted_ball_data = [\n",
    "    #des,\n",
    "    #game_type,\n",
    "    #'events',\n",
    "    'stand',\n",
    "    'p_throws',\n",
    "    'home_team',\n",
    "    #away_team,\n",
    "    #type,\n",
    "    #'hit_location,\n",
    "    #'hc_x',\n",
    "    #'hc_y',\n",
    "    'launch_speed',\n",
    "    'launch_angle',\n",
    "    'field_angle'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a categorical variable for home runs.\n",
    "df['is_homer'] = df['events'].apply(lambda x: True if x == 'home_run' else False)\n",
    "df['stand'] = df['stand'].apply(lambda x: True if x == 'R' else False)\n",
    "df['p_throws'] = df['p_throws'].apply(lambda x: True if x == 'R' else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hc_x and hc_y are unusual. An internet search suggests that they are coordinates for\n",
    "##displying the hit location on a field map for MLB's app.\n",
    "##That's obvious data leakage, but I do want the relative angle that these numbers can imply.\n",
    "\n",
    "## Running the code below shows that they aren't scaled in the same way, both have a minimum of 0.\n",
    "\n",
    "df[df['events'] == 'home_run'][['hc_x','hc_y']].dropna().sort_values(by='hc_x', ascending=False)\n",
    "\n",
    "## Scale these\n",
    "## I'm assuming that the max of hc_y is about the same actual distance as the max of hc_x. \n",
    "df['hc_x'] = df['hc_x'] / df['hc_x'].max()\n",
    "df['hc_y'] = df['hc_y'] / df['hc_y'].max()\n",
    "df['field_angle'] = np.arctan(df['hc_y']/df['hc_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0512\nDummy score: 0.9064\nTrain: 0.9784\nTest: 0.968\nCofusion:\n[[4684   60]\n [ 100  156]]\n"
     ]
    }
   ],
   "source": [
    "## Before going any further, I want to see how well a basic model\n",
    "## can classify this data based on the most obvious features: launch angle and launch speed\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "## Get small test and train splits to train a decision tree.\n",
    "temp = df.dropna(subset=['launch_angle','launch_speed','is_homer'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp[['launch_angle','launch_speed']], \n",
    "                                                    temp['is_homer'], \n",
    "                                                    test_size=5_000, \n",
    "                                                    train_size=10_000,\n",
    "                                                    random_state = 12\n",
    "                                                    )\n",
    "\n",
    "baseline_clf = DecisionTreeClassifier(max_depth = 8).fit(X_train,y_train)\n",
    "dummy_clf = DummyClassifier().fit(X_train, y_train)\n",
    "print(y_test.sum()/len(y_test))\n",
    "print(\"Dummy score: {}\".format(dummy_clf.score(X_test,y_test)))\n",
    "print(\"Train: {}\".format(baseline_clf.score(X_train,y_train)))\n",
    "print(\"Test: {}\".format(baseline_clf.score(X_test,y_test)))\n",
    "print(\"Cofusion:\\n{}\".format(confusion_matrix(y_test,baseline_clf.predict(X_test))))\n"
   ]
  },
  {
   "source": [
    "#### That was surprisingly bad\n",
    "\n",
    "I though a decision tree would do better than that, but it only correctly classifies 156 of 216 home runs, which is a pretty weak recall rate. Precision is even worse, with 256 classifications (about .6) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "temp = df.dropna(subset =batted_ball_data)\n",
    "temp = temp[batted_ball_data+['is_homer']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp[batted_ball_data], \n",
    "                                                    temp['is_homer'], \n",
    "                                                    test_size=5_000, \n",
    "                                                    train_size=10_000,\n",
    "                                                    random_state = 12\n",
    "                                                    )\n",
    "\n",
    "col_transf = ColumnTransformer([\n",
    "    ('one_hot',OneHotEncoder(sparse=False),['home_team'])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "tree_clf_pipe = Pipeline(\n",
    "    [('column_transformer', col_transf),\n",
    "    ('tree_clf', DecisionTreeClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0556\nDummy score: 0.899\nTrain: 1.0\nTest: 0.9658\nCofusion:\n[[4649   73]\n [  98  180]]\n"
     ]
    }
   ],
   "source": [
    "tree_clf_pipe.fit(X_train,y_train)\n",
    "\n",
    "dummy_clf = DummyClassifier().fit(X_train, y_train)\n",
    "print(y_test.sum()/len(y_test))\n",
    "print(\"Dummy score: {}\".format(dummy_clf.score(X_test,y_test)))\n",
    "print(\"Train: {}\".format(tree_clf_pipe.score(X_train,y_train)))\n",
    "print(\"Test: {}\".format(tree_clf_pipe.score(X_test,y_test)))\n",
    "print(\"Cofusion:\\n{}\".format(confusion_matrix(y_test,tree_clf_pipe.predict(X_test))))\n",
    "\n",
    "## Let's turn that into something quicker to re-use.\n",
    "def performance_summary(model,X,y,X_dev,y_dev):\n",
    "    \n",
    "    dummy_clf = DummyClassifier().fit(X, y)\n",
    "    print(y_dev.sum()/len(y_dev))\n",
    "    print(\"Dummy score: {}\".format(dummy_clf.score(X_dev,y_dev)))\n",
    "    print(\"Train: {}\".format(model.score(X,y)))\n",
    "    print(\"Test: {}\".format(model.score(X_dev,y_dev)))\n",
    "    print(\"Train Cofusion:\\n{}\".format(confusion_matrix(y,model.predict(X))))\n",
    "    print(\"Cofusion:\\n{}\".format(confusion_matrix(y_dev,model.predict(X_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-157-19f8e52c14e1>, line 8)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-157-19f8e52c14e1>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    learning_rate_init = .03,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_6 = MLPClassifier(\n",
    "    hidden_layer_sizes=(6),\n",
    "    activation = 'relu',\n",
    "    max_iter = 1000,\n",
    "    solver = 'sgd'\n",
    "    learning_rate_init = .03,\n",
    "    alpha = 0\n",
    ")\n",
    "\n",
    "col_transf = ColumnTransformer([\n",
    "    ('one_hot',OneHotEncoder(sparse=False),['home_team']),\n",
    "    ('scaler',StandardScaler(),['launch_speed','launch_angle','field_angle'])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "mlp_6_pipe = Pipeline(\n",
    "    [('column_transformer', col_transf),\n",
    "    ('mlp_clf', mlp_6)]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('column_transformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('one_hot',\n",
       "                                                  OneHotEncoder(sparse=False),\n",
       "                                                  ['home_team']),\n",
       "                                                 ('scaler', StandardScaler(),\n",
       "                                                  ['launch_speed',\n",
       "                                                   'launch_angle',\n",
       "                                                   'field_angle'])])),\n",
       "                ('mlp_clf',\n",
       "                 MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                               learning_rate_init=0.03, max_iter=1000))])"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "mlp_6_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0556\nDummy score: 0.895\nTrain: 0.9791510745215439\nTest: 0.9752\nCofusion:\n[[4676   46]\n [  78  200]]\n"
     ]
    }
   ],
   "source": [
    "performance_summary(mlp_6_pipe,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "source": [
    "## First MLP does about the same as a decision tree (but overfits less.)\n",
    "\n",
    "Let's try giving it more data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        stand  p_throws home_team  launch_speed  launch_angle  field_angle\n",
       "388651   True      True        SF          76.2          26.0     0.611815\n",
       "647539   True      True       LAD         102.0          -3.0     1.004972\n",
       "231292   True      True        SF          82.9         -21.0     1.037215\n",
       "621458  False      True       STL         103.4          14.0     0.453784\n",
       "275326   True     False       CLE          82.9         -21.0     1.025046\n",
       "...       ...       ...       ...           ...           ...          ...\n",
       "220006   True     False       CIN          86.4          37.0     0.474307\n",
       "241604   True      True       NYM          89.2          23.0     0.734034\n",
       "121153  False      True       STL          94.9          30.0     0.377005\n",
       "616181   True      True       CIN          90.0          15.0     0.744305\n",
       "472578   True      True       CWS          84.5         -13.0     1.048430\n",
       "\n",
       "[113603 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stand</th>\n      <th>p_throws</th>\n      <th>home_team</th>\n      <th>launch_speed</th>\n      <th>launch_angle</th>\n      <th>field_angle</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>388651</th>\n      <td>True</td>\n      <td>True</td>\n      <td>SF</td>\n      <td>76.2</td>\n      <td>26.0</td>\n      <td>0.611815</td>\n    </tr>\n    <tr>\n      <th>647539</th>\n      <td>True</td>\n      <td>True</td>\n      <td>LAD</td>\n      <td>102.0</td>\n      <td>-3.0</td>\n      <td>1.004972</td>\n    </tr>\n    <tr>\n      <th>231292</th>\n      <td>True</td>\n      <td>True</td>\n      <td>SF</td>\n      <td>82.9</td>\n      <td>-21.0</td>\n      <td>1.037215</td>\n    </tr>\n    <tr>\n      <th>621458</th>\n      <td>False</td>\n      <td>True</td>\n      <td>STL</td>\n      <td>103.4</td>\n      <td>14.0</td>\n      <td>0.453784</td>\n    </tr>\n    <tr>\n      <th>275326</th>\n      <td>True</td>\n      <td>False</td>\n      <td>CLE</td>\n      <td>82.9</td>\n      <td>-21.0</td>\n      <td>1.025046</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>220006</th>\n      <td>True</td>\n      <td>False</td>\n      <td>CIN</td>\n      <td>86.4</td>\n      <td>37.0</td>\n      <td>0.474307</td>\n    </tr>\n    <tr>\n      <th>241604</th>\n      <td>True</td>\n      <td>True</td>\n      <td>NYM</td>\n      <td>89.2</td>\n      <td>23.0</td>\n      <td>0.734034</td>\n    </tr>\n    <tr>\n      <th>121153</th>\n      <td>False</td>\n      <td>True</td>\n      <td>STL</td>\n      <td>94.9</td>\n      <td>30.0</td>\n      <td>0.377005</td>\n    </tr>\n    <tr>\n      <th>616181</th>\n      <td>True</td>\n      <td>True</td>\n      <td>CIN</td>\n      <td>90.0</td>\n      <td>15.0</td>\n      <td>0.744305</td>\n    </tr>\n    <tr>\n      <th>472578</th>\n      <td>True</td>\n      <td>True</td>\n      <td>CWS</td>\n      <td>84.5</td>\n      <td>-13.0</td>\n      <td>1.048430</td>\n    </tr>\n  </tbody>\n</table>\n<p>113603 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "## We up the training data to the full subset.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp[batted_ball_data], \n",
    "                                                    temp['is_homer'], \n",
    "                                                    test_size=10_000, \n",
    "                                                    ##train_size=10_000,\n",
    "                                                    random_state = 12\n",
    "                                                    )\n",
    "X_test, X_final_test, y_test, y_final_test = train_test_split(X_test, y_test, test_size = 5000)##I know the trandional is X_dev and X_test but I didn't start out this way\n",
    "mlp_6_pipe.fit(X_train,y_train)\n",
    "performance_summary(mlp_6_pipe,X_train,y_train,X_test,y_test)\n",
    "\n",
    "## Fun, no so fun side note: on my first pass with more data, there was a total failure: \n",
    "## it predicted the majority class. (Uhg.)\n",
    "## I forgot to scale the data, which immediately eliminated the issue.\n"
   ]
  },
  {
   "source": [
    "### PSA\n",
    "\n",
    "I ran the following with a subset of the data, assuming that the results on a large (~25,000) training set would translate to the full set. They did not. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                                     learning_rate_init=0.03, max_iter=1000),\n",
       "             param_grid={'hidden_layer_sizes': [40, 100, (60, 20)]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "## Let's see if a bigger network can do more...\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k_fold = KFold(n_splits=2) #saves time; let's find something that looks promising, then get more rigorous.\n",
    "\n",
    "parameters = {'hidden_layer_sizes':[40,100,(60,20)]}\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([12.81085801, 13.76236391, 23.18039608]),\n",
       " 'std_fit_time': array([ 1.07606316,  0.21776295, 13.92780542]),\n",
       " 'mean_score_time': array([0.08179283, 0.10742295, 0.09744596]),\n",
       " 'std_score_time': array([0.00201464, 0.00162661, 0.00010395]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[40, 100, (60, 20)],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': 40},\n",
       "  {'hidden_layer_sizes': 100},\n",
       "  {'hidden_layer_sizes': (60, 20)}],\n",
       " 'split0_test_score': array([0.96963135, 0.85178339, 0.96670892]),\n",
       " 'split1_test_score': array([0.94677911, 0.96480696, 0.94677911]),\n",
       " 'mean_test_score': array([0.95820523, 0.90829517, 0.95674401]),\n",
       " 'std_test_score': array([0.01142612, 0.05651178, 0.00996491]),\n",
       " 'rank_test_score': array([1, 3, 2], dtype=int32),\n",
       " 'split0_train_score': array([0.970247  , 0.85206246, 0.9676062 ]),\n",
       " 'split1_train_score': array([0.94635752, 0.96498363, 0.94635752]),\n",
       " 'mean_train_score': array([0.95830226, 0.90852305, 0.95698186]),\n",
       " 'std_train_score': array([0.01194474, 0.05646058, 0.01062434])}"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "## A larger network slightly over fits but does better on the training set.\n",
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                                     learning_rate_init=0.03, max_iter=1000),\n",
       "             param_grid={'hidden_layer_sizes': [(50, 20), (80, 30), (100, 40)]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "## Let's try a deeper network. See if we can really capture the training data.\n",
    "\n",
    "\n",
    "parameters = {'hidden_layer_sizes':[(50,20),(80,30),(100,40)]}\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([16.54689515, 21.94701886, 16.95467925]),\n",
       " 'std_fit_time': array([9.20941985, 1.09892941, 7.84905624]),\n",
       " 'mean_score_time': array([0.09131181, 0.1143589 , 0.13669407]),\n",
       " 'std_score_time': array([0.00076663, 0.000144  , 0.00514138]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(50, 20), (80, 30), (100, 40)],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': (50, 20)},\n",
       "  {'hidden_layer_sizes': (80, 30)},\n",
       "  {'hidden_layer_sizes': (100, 40)}],\n",
       " 'split0_test_score': array([0.94635752, 0.97135664, 0.94635752]),\n",
       " 'split1_test_score': array([0.94677911, 0.94677911, 0.91464939]),\n",
       " 'mean_test_score': array([0.94656831, 0.95906787, 0.93050346]),\n",
       " 'std_test_score': array([0.00021079, 0.01228877, 0.01585407]),\n",
       " 'rank_test_score': array([2, 1, 3], dtype=int32),\n",
       " 'split0_train_score': array([0.94677911, 0.97271175, 0.94677911]),\n",
       " 'split1_train_score': array([0.94635752, 0.94635752, 0.91484455]),\n",
       " 'mean_train_score': array([0.94656831, 0.95953464, 0.93081183]),\n",
       " 'std_train_score': array([0.00021079, 0.01317711, 0.01596728])}"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                                     learning_rate_init=0.03, max_iter=1000),\n",
       "             param_grid={'hidden_layer_sizes': [(80, 30), (100, 40), (200, 50)],\n",
       "                         'learning_rate_init': [0.01, 0.03],\n",
       "                         'random_state': [5]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "## Why not?\n",
    "# \n",
    "parameters = {'hidden_layer_sizes':[(80,30),(100,40),(200,50)],\n",
    "            #'learning_rate':['invscaling'],\n",
    "            'learning_rate_init':[.01,.03],\n",
    "            'random_state':[5]}\n",
    "k_fold.n_splits = 3 ## Let's make sure these are getting about the same results.\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([34.2814366 , 41.40813033, 51.08162777, 32.4341588 , 71.04437359,\n",
       "        61.21731393]),\n",
       " 'std_fit_time': array([10.44313809, 17.6699185 , 24.88505709,  3.59838792, 23.61255725,\n",
       "        26.48859119]),\n",
       " 'mean_score_time': array([0.07689341, 0.07464798, 0.08761986, 0.08658218, 0.13836193,\n",
       "        0.13369068]),\n",
       " 'std_score_time': array([0.00051584, 0.00180158, 0.00238551, 0.00129058, 0.00291153,\n",
       "        0.00052116]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(80, 30), (80, 30), (100, 40), (100, 40), (200, 50),\n",
       "                    (200, 50)],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[0.01, 0.03, 0.01, 0.03, 0.01, 0.03],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_random_state': masked_array(data=[5, 5, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': (80, 30),\n",
       "   'learning_rate_init': 0.01,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (80, 30),\n",
       "   'learning_rate_init': 0.03,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (100, 40),\n",
       "   'learning_rate_init': 0.01,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (100, 40),\n",
       "   'learning_rate_init': 0.03,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (200, 50),\n",
       "   'learning_rate_init': 0.01,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (200, 50),\n",
       "   'learning_rate_init': 0.03,\n",
       "   'random_state': 5}],\n",
       " 'split0_test_score': array([0.97216647, 0.97100454, 0.96870709, 0.96477237, 0.97229851,\n",
       "        0.96477237]),\n",
       " 'split1_test_score': array([0.97322277, 0.97229851, 0.97290588, 0.97319637, 0.97441111,\n",
       "        0.97264181]),\n",
       " 'split2_test_score': array([0.9681781 , 0.96778197, 0.97221855, 0.96036126, 0.9704492 ,\n",
       "        0.96820451]),\n",
       " 'mean_test_score': array([0.97118911, 0.97036168, 0.97127718, 0.96611   , 0.97238628,\n",
       "        0.96853956]),\n",
       " 'std_test_score': array([0.00217234, 0.00189908, 0.00183886, 0.00532459, 0.00161863,\n",
       "        0.00322141]),\n",
       " 'rank_test_score': array([3, 4, 2, 6, 1, 5], dtype=int32),\n",
       " 'split0_train_score': array([0.97311679, 0.97184921, 0.96943289, 0.96693735, 0.97503136,\n",
       "        0.96523404]),\n",
       " 'split1_train_score': array([0.97290553, 0.97138707, 0.97178319, 0.972483  , 0.97360533,\n",
       "        0.97231135]),\n",
       " 'split2_train_score': array([0.970516  , 0.96817894, 0.97298511, 0.96062639, 0.97138745,\n",
       "        0.96898437]),\n",
       " 'mean_train_score': array([0.97217944, 0.97047174, 0.9714004 , 0.96668224, 0.97334138,\n",
       "        0.96884325]),\n",
       " 'std_train_score': array([0.00117938, 0.0016322 , 0.00147523, 0.0048438 , 0.00149928,\n",
       "        0.00289102])}"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "mlp_grid.cv_results_"
   ]
  },
  {
   "source": [
    "#### Let's go with 100,40 for the hidden layers.\n",
    "\n",
    "If increasing the amount of data creates gains over the smaller networks, we should be able to capture that. Training time was about the same for 100, (100,20) and (100,40).\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-3cb6278fe552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmlp_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmlp_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_transf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = {'hidden_layer_sizes':[(200,80)],\n",
    "            #'learning_rate':['invscaling'],\n",
    "            'learning_rate_init':[.005],#,.01,.02],\n",
    "            'random_state':[5]}\n",
    "k_fold.n_splits = 3 ## Let's make sure these are getting about the same results.\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "\n",
    "start = time.time()\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 60.68581128, 134.45602242, 123.82021133]),\n",
       " 'std_fit_time': array([46.8980901 , 32.20335154, 16.36133001]),\n",
       " 'mean_score_time': array([0.18083231, 0.16009402, 0.16086682]),\n",
       " 'std_score_time': array([0.01082454, 0.00083109, 0.00432355]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(200, 80), (200, 80), (200, 80)],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[0.05, 0.01, 0.02],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_random_state': masked_array(data=[5, 5, 5],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': (200, 80),\n",
       "   'learning_rate_init': 0.05,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (200, 80),\n",
       "   'learning_rate_init': 0.01,\n",
       "   'random_state': 5},\n",
       "  {'hidden_layer_sizes': (200, 80),\n",
       "   'learning_rate_init': 0.02,\n",
       "   'random_state': 5}],\n",
       " 'split0_test_score': array([0.94578536, 0.97216647, 0.96947291]),\n",
       " 'split1_test_score': array([0.94800359, 0.97419985, 0.973302  ]),\n",
       " 'split2_test_score': array([0.96730663, 0.97277313, 0.97258827]),\n",
       " 'mean_test_score': array([0.95369853, 0.97304648, 0.97178772]),\n",
       " 'std_test_score': array([0.0096649 , 0.00085233, 0.00166256]),\n",
       " 'rank_test_score': array([3, 1, 2], dtype=int32),\n",
       " 'split0_train_score': array([0.94695979, 0.97427874, 0.9704628 ]),\n",
       " 'split1_train_score': array([0.94585066, 0.97359213, 0.97199445]),\n",
       " 'split2_train_score': array([0.96815253, 0.97404141, 0.97343403]),\n",
       " 'mean_train_score': array([0.95365433, 0.97397076, 0.97196376]),\n",
       " 'std_train_score': array([0.01026177, 0.00028472, 0.0012132 ])}"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_100_40 = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,40),\n",
    "    activation = 'relu',\n",
    "    max_iter = 1000,\n",
    "    learning_rate_init = .03,\n",
    "    alpha = 0\n",
    ")\n",
    "\n",
    "mlp_100_40_pipe = Pipeline(\n",
    "    [('column_transformer', col_transf),\n",
    "    ('mlp_clf', mlp_100_40)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0506\n",
      "Dummy score: 0.9016\n",
      "Train: 0.9465683124565373\n",
      "Test: 0.9494\n",
      "Train Cofusion:\n",
      "[[107533      0]\n",
      " [  6070      0]]\n",
      "Cofusion:\n",
      "[[4747    0]\n",
      " [ 253    0]]\n"
     ]
    }
   ],
   "source": [
    "mlp_100_40_pipe.fit(X_train,y_train)\n",
    "performance_summary(mlp_100_40_pipe,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}