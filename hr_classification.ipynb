{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbaseconda1fad7997e8f94ea293ffe5b4ecbfb0c4",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the necessities\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "## Load sklearn modules\n",
    "\n",
    "### model selection and evaluation modules\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "### pre-processing and pipeline steps\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### estimators\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Are you checking out my code?\n",
    "\n",
    "The csv data is scraped from Statcast. The datafiles are huge (about 350MB each) and not on my github repository. You can use the following cells \n",
    "to test the functionality of the code yourself, either by a scraper to download the above data or by using the data subset file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commnet out the last lines in this cell, then uncomment an option below to run this notebook.\n",
    "\n",
    "\n",
    "## OPTION 1:\n",
    "## Slow, but complete.\n",
    "## Uncomment and run the following code to scrape a file identical to the one used above to verify this code as-is.\n",
    "\n",
    "#import datetime\n",
    "#import scraper ##this is one of my files, found in the same repo as this ipynb.\n",
    "\n",
    "#start = datetime.date(2019,3,20)\n",
    "#end = datetime.date(2019,10,1)\n",
    "#df1 = scraper.statcast_scrape(start,end)\n",
    "#df = df1.dropna(subset=['events'])\n",
    "\n",
    "\n",
    "## OPTION 2: \n",
    "## Fast, but the models will be fitting a smaller amount of data and your results may vary\n",
    "## from mine if you do.\n",
    "## The 'statcast2019_sample.csv' was produced with DataFrame.sample(50_000, random_state = 1). It is smaller and on github.\n",
    "## Uncomment below to read that file as sample data to inspect this code.\n",
    "\n",
    "#df = pd.read_csv('data/statcast2019_sample.csv')\n",
    "\n",
    "## OPTION 3: \n",
    "## You already have these files, saved in the right way. (99% chance you are me, if so.)\n",
    "## This code occasionally crashes my old, slow computer. Hence the pickling (which is also faster.)\n",
    "\n",
    "#df = pd.read_csv('data/statcast_dumps/statcast2019.csv').dropna(subset=['events'])\n",
    "#df\n",
    "#all_data = df.copy()\n",
    "#df = df.sample(n=50_000)\n",
    "\n",
    "##So I'm creating a pickle to replace it.\n",
    "\n",
    "#pickle_out = open('pickles/hr_clf_data_subset.pickle','wb')\n",
    "#pickle.dump(df, pickle_out)\n",
    "\n",
    "##And now I can load that data with\n",
    "\n",
    "pickle_in = open('pickles/hr_clf_data_subset.pickle','rb')\n",
    "df = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'Unnamed: 0'',\n'index'',\n'pitch_type'',\n'game_date'',\n'release_speed'',\n'release_pos_x'',\n'release_pos_z'',\n'player_name'',\n'batter'',\n'pitcher'',\n'events'',\n'description'',\n'spin_dir'',\n'spin_rate_deprecated'',\n'break_angle_deprecated'',\n'break_length_deprecated'',\n'zone'',\n'des'',\n'game_type'',\n'stand'',\n'p_throws'',\n'home_team'',\n'away_team'',\n'type'',\n'hit_location'',\n'bb_type'',\n'balls'',\n'strikes'',\n'game_year'',\n'pfx_x'',\n'pfx_z'',\n'plate_x'',\n'plate_z'',\n'on_3b'',\n'on_2b'',\n'on_1b'',\n'outs_when_up'',\n'inning'',\n'inning_topbot'',\n'hc_x'',\n'hc_y'',\n'tfs_deprecated'',\n'tfs_zulu_deprecated'',\n'fielder_2'',\n'umpire'',\n'sv_id'',\n'vx0'',\n'vy0'',\n'vz0'',\n'ax'',\n'ay'',\n'az'',\n'sz_top'',\n'sz_bot'',\n'hit_distance_sc'',\n'launch_speed'',\n'launch_angle'',\n'effective_speed'',\n'release_spin_rate'',\n'release_extension'',\n'game_pk'',\n'pitcher.1'',\n'fielder_2.1'',\n'fielder_3'',\n'fielder_4'',\n'fielder_5'',\n'fielder_6'',\n'fielder_7'',\n'fielder_8'',\n'fielder_9'',\n'release_pos_y'',\n'estimated_ba_using_speedangle'',\n'estimated_woba_using_speedangle'',\n'woba_value'',\n'woba_denom'',\n'babip_value'',\n'iso_value'',\n'launch_speed_angle'',\n'at_bat_number'',\n'pitch_number'',\n'pitch_name'',\n'home_score'',\n'away_score'',\n'bat_score'',\n'fld_score'',\n'post_away_score'',\n'post_home_score'',\n'post_bat_score'',\n'post_fld_score'',\n'if_fielding_alignment'',\n'of_fielding_alignment'',\n"
     ]
    }
   ],
   "source": [
    "#print(df['des'].unique())\n",
    "for i in df.columns:\n",
    "    print(\"'{}'',\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14335     Adalberto Mondesi hits an inside-the-park home...\n",
       "200750    George Springer hits an inside-the-park home r...\n",
       "203754    Kevin Kiermaier hits an inside-the-park home r...\n",
       "223013    Avisail Garcia hits an inside-the-park home ru...\n",
       "289952    Hunter Pence hits an inside-the-park home run ...\n",
       "318774    Ian Desmond hits an inside-the-park home run (...\n",
       "348009    Ben Gamel hits an inside-the-park home run (5)...\n",
       "356763    Tommy La Stella hits an inside-the-park home r...\n",
       "415030    Royals challenged (tag play), call on the fiel...\n",
       "471236    Yuli Gurriel hits an inside-the-park home run ...\n",
       "503374    Ketel Marte hits an inside-the-park home run (...\n",
       "524797    Whit Merrifield hits an inside-the-park home r...\n",
       "633839    Scott Kingery hits an inside-the-park home run...\n",
       "Name: des, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#Checking for in the park HR.\n",
    "\n",
    "df[df['events'] == 'home_run'].dropna(subset=['hit_location'])['des']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a list of batted ball data. I'm also including batter handedness and home team (which is a proxy for the park.)\n",
    "batted_ball_data = [\n",
    "    #des,\n",
    "    #game_type,\n",
    "    #'events',\n",
    "    'stand',\n",
    "    'p_throws',\n",
    "    'home_team',\n",
    "    #away_team,\n",
    "    #type,\n",
    "    #'hit_location,\n",
    "    #'hc_x',\n",
    "    #'hc_y',\n",
    "    'launch_speed',\n",
    "    'launch_angle',\n",
    "    'field_angle'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a categorical variable for home runs.\n",
    "df['is_homer'] = df['events'].apply(lambda x: True if x == 'home_run' else False)\n",
    "df['stand'] = df['stand'].apply(lambda x: True if x == 'R' else False)\n",
    "df['p_throws'] = df['p_throws'].apply(lambda x: True if x == 'R' else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hc_x and hc_y are unusual. An internet search suggests that they are coordinates for\n",
    "##displying the hit location on a field map for MLB's app.\n",
    "##That's obvious data leakage, but I do want the relative angle that these numbers can imply.\n",
    "\n",
    "## Running the code below shows that they aren't scaled in the same way, both have a minimum of 0.\n",
    "\n",
    "df[df['events'] == 'home_run'][['hc_x','hc_y']].dropna().sort_values(by='hc_x', ascending=False)\n",
    "\n",
    "## Scale these\n",
    "## I'm assuming that the max of hc_y is about the same actual distance as the max of hc_x. \n",
    "df['hc_x'] = df['hc_x'] / df['hc_x'].max()\n",
    "df['hc_y'] = df['hc_y'] / df['hc_y'].max()\n",
    "df['field_angle'] = np.arctan(df['hc_y']/df['hc_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.057\nDummy score: 0.8968\nTrain: 0.9792\nTest: 0.9638\nCofusion:\n[[4653   62]\n [ 119  166]]\n"
     ]
    }
   ],
   "source": [
    "## Before going any further, I want to see how well a basic model\n",
    "## can classify this data based on the most obvious features: launch angle and launch speed\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "## Get small test and train splits to train a decision tree.\n",
    "temp = df.dropna(subset=['launch_angle','launch_speed','is_homer'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp[['launch_angle','launch_speed']], \n",
    "                                                    temp['is_homer'], \n",
    "                                                    test_size=5_000, \n",
    "                                                    train_size=10_000,\n",
    "                                                    random_state = 12\n",
    "                                                    )\n",
    "\n",
    "baseline_clf = DecisionTreeClassifier(max_depth = 8).fit(X_train,y_train)\n",
    "dummy_clf = DummyClassifier().fit(X_train, y_train)\n",
    "print(y_test.sum()/len(y_test))\n",
    "print(\"Dummy score: {}\".format(dummy_clf.score(X_test,y_test)))\n",
    "print(\"Train: {}\".format(baseline_clf.score(X_train,y_train)))\n",
    "print(\"Test: {}\".format(baseline_clf.score(X_test,y_test)))\n",
    "print(\"Cofusion:\\n{}\".format(confusion_matrix(y_test,baseline_clf.predict(X_test))))\n"
   ]
  },
  {
   "source": [
    "#### That was surprisingly bad\n",
    "\n",
    "I though a decision tree would do better than that, but it only correctly classifies 156 of 216 home runs, which is a pretty weak recall rate. Precision is even worse, with 256 classifications (about .6) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.dropna(subset =batted_ball_data) ## variables named 'temp' are always temporary and not expected to be used in later cells.\n",
    "temp = temp[batted_ball_data+['is_homer']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp[batted_ball_data], \n",
    "                                                    temp['is_homer'], \n",
    "                                                    test_size=5_000, \n",
    "                                                    train_size=10_000,\n",
    "                                                    random_state = 12\n",
    "                                                    )\n",
    "\n",
    "col_transf = ColumnTransformer([\n",
    "    ('one_hot',OneHotEncoder(sparse=False),['home_team'])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "tree_clf_pipe = Pipeline(\n",
    "    [('column_transformer', col_transf),\n",
    "    ('tree_clf', DecisionTreeClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0504\nDummy score: 0.9054\nTrain: 1.0\nTest: 0.9648\nCofusion:\n[[4653   95]\n [  81  171]]\n"
     ]
    }
   ],
   "source": [
    "tree_clf_pipe.fit(X_train,y_train)\n",
    "\n",
    "dummy_clf = DummyClassifier().fit(X_train, y_train)\n",
    "print(y_test.sum()/len(y_test))\n",
    "print(\"Dummy score: {}\".format(dummy_clf.score(X_test,y_test)))\n",
    "print(\"Train: {}\".format(tree_clf_pipe.score(X_train,y_train)))\n",
    "print(\"Test: {}\".format(tree_clf_pipe.score(X_test,y_test)))\n",
    "print(\"Cofusion:\\n{}\".format(confusion_matrix(y_test,tree_clf_pipe.predict(X_test))))\n",
    "\n",
    "## Let's turn that into something quicker to re-use.\n",
    "def performance_summary(model,X,y,X_dev,y_dev):\n",
    "    \n",
    "    dummy_clf = DummyClassifier().fit(X, y)\n",
    "    print(y_dev.sum()/len(y_dev))\n",
    "    print(\"Dummy score: {}\".format(dummy_clf.score(X_dev,y_dev)))\n",
    "    print(\"Train: {}\".format(model.score(X,y)))\n",
    "    print(\"Test: {}\".format(model.score(X_dev,y_dev)))\n",
    "    print(\"Train Cofusion:\\n{}\".format(confusion_matrix(y,model.predict(X))))\n",
    "    print(\"Cofusion:\\n{}\".format(confusion_matrix(y_dev,model.predict(X_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp_6 = MLPClassifier(\n",
    "    hidden_layer_sizes=(6),\n",
    "    activation = 'relu',\n",
    "    max_iter = 1000,\n",
    "    solver = 'sgd',\n",
    "    learning_rate_init = .03,\n",
    "    alpha = 0\n",
    ")\n",
    "\n",
    "col_transf = ColumnTransformer([\n",
    "    ('one_hot',OneHotEncoder(sparse=False),['home_team']),\n",
    "    ('scaler',StandardScaler(),['launch_speed','launch_angle','field_angle'])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "mlp_6_pipe = Pipeline(\n",
    "    [('column_transformer', col_transf),\n",
    "    ('mlp_clf', mlp_6)]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('column_transformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('one_hot',\n",
       "                                                  OneHotEncoder(sparse=False),\n",
       "                                                  ['home_team']),\n",
       "                                                 ('scaler', StandardScaler(),\n",
       "                                                  ['launch_speed',\n",
       "                                                   'launch_angle',\n",
       "                                                   'field_angle'])])),\n",
       "                ('mlp_clf',\n",
       "                 MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                               learning_rate_init=0.03, max_iter=1000,\n",
       "                               solver='sgd'))])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "mlp_6_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0504\nDummy score: 0.9016\nTrain: 0.981\nTest: 0.9742\nTrain Cofusion:\n[[9385  102]\n [  88  425]]\nCofusion:\n[[4676   72]\n [  57  195]]\n"
     ]
    }
   ],
   "source": [
    "performance_summary(mlp_6_pipe,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "source": [
    "## First MLP does about the same as a decision tree (but overfits less.)\n",
    "\n",
    "Let's try giving it more data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0502\n",
      "Dummy score: 0.9096\n",
      "Train: 0.9773509502389902\n",
      "Test: 0.9792\n",
      "Train Cofusion:\n",
      "[[106278   1255]\n",
      " [  1318   4752]]\n",
      "Cofusion:\n",
      "[[4702   47]\n",
      " [  57  194]]\n"
     ]
    }
   ],
   "source": [
    "## We up the training data to the full set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp[batted_ball_data], \n",
    "                                                    temp['is_homer'], \n",
    "                                                    test_size=10_000, \n",
    "                                                    ##train_size=10_000,\n",
    "                                                    random_state = 12\n",
    "                                                    )\n",
    "X_test, X_final_test, y_test, y_final_test = train_test_split(X_test, y_test, test_size = 5000)##I know the trandional is X_dev and X_test but I didn't start out this way\n",
    "mlp_6_pipe.fit(X_train,y_train)\n",
    "performance_summary(mlp_6_pipe,X_train,y_train,X_test,y_test)\n",
    "\n",
    "## Fun, no so fun side note: on my first pass with more data, there was a total failure: \n",
    "## it predicted the majority class. (Uhg.)\n",
    "## I forgot to scale the data, which immediately eliminated the issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                                     learning_rate_init=0.03, max_iter=1000,\n",
       "                                     solver='sgd'),\n",
       "             param_grid={'hidden_layer_sizes': [6, 10, 20]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "## Let's see if a bigger network can do more...\n",
    "\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=2) #saves time; let's find something that looks promising, then get more rigorous.\n",
    "\n",
    "parameters = {'hidden_layer_sizes':[6,10,20]}\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([14.41100788, 14.17461848, 24.47932947]),\n",
       " 'std_fit_time': array([4.05109358, 1.9933486 , 8.08766687]),\n",
       " 'mean_score_time': array([0.01457548, 0.0177834 , 0.02378428]),\n",
       " 'std_score_time': array([0.00104523, 0.00062966, 0.00161564]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[6, 10, 20],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': 6},\n",
       "  {'hidden_layer_sizes': 10},\n",
       "  {'hidden_layer_sizes': 20}],\n",
       " 'split0_test_score': array([0.9743671 , 0.97508891, 0.97635647]),\n",
       " 'split1_test_score': array([0.97503565, 0.97649689, 0.97720111]),\n",
       " 'mean_test_score': array([0.97470138, 0.9757929 , 0.97677879]),\n",
       " 'std_test_score': array([0.00033428, 0.00070399, 0.00042232]),\n",
       " 'rank_test_score': array([3, 2, 1], dtype=int32),\n",
       " 'split0_train_score': array([0.97702505, 0.97769405, 0.97975388]),\n",
       " 'split1_train_score': array([0.97581071, 0.97582832, 0.97776487]),\n",
       " 'mean_train_score': array([0.97641788, 0.97676119, 0.97875937]),\n",
       " 'std_train_score': array([0.00060717, 0.00093287, 0.00099451])}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "## A larger network slightly over fits but does better on the training set.\n",
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                                     learning_rate_init=0.03, max_iter=1000,\n",
       "                                     solver='sgd'),\n",
       "             param_grid={'hidden_layer_sizes': [(20, 8), (30, 10), (40, 12)],\n",
       "                         'learning_rate_init': [0.01, 0.005]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "## Let's try a deeper network. See if we can really capture the training data.\n",
    "\n",
    "\n",
    "parameters = {'hidden_layer_sizes':[(20,8),(30,10),(40,12)],\n",
    "                'learning_rate_init' : [.01, .005]} #lower the learning rate. In my experience, deeper networks need to learn a little slower.\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([29.85540533, 25.91176689, 21.42548037, 36.91480339, 46.65487754,\n",
       "        37.54543984]),\n",
       " 'std_fit_time': array([4.81753135, 2.44942605, 5.53138924, 0.11891854, 9.84136808,\n",
       "        4.8721987 ]),\n",
       " 'mean_score_time': array([0.03223228, 0.0301404 , 0.03638661, 0.03563738, 0.04789138,\n",
       "        0.04614401]),\n",
       " 'std_score_time': array([0.00418973, 0.00170326, 0.00021374, 0.0006547 , 0.00229836,\n",
       "        0.00048256]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(20, 8), (20, 8), (30, 10), (30, 10), (40, 12),\n",
       "                    (40, 12)],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[0.01, 0.005, 0.01, 0.005, 0.01, 0.005],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': (20, 8), 'learning_rate_init': 0.01},\n",
       "  {'hidden_layer_sizes': (20, 8), 'learning_rate_init': 0.005},\n",
       "  {'hidden_layer_sizes': (30, 10), 'learning_rate_init': 0.01},\n",
       "  {'hidden_layer_sizes': (30, 10), 'learning_rate_init': 0.005},\n",
       "  {'hidden_layer_sizes': (40, 12), 'learning_rate_init': 0.01},\n",
       "  {'hidden_layer_sizes': (40, 12), 'learning_rate_init': 0.005}],\n",
       " 'split0_test_score': array([0.97596916, 0.97433189, 0.97621563, 0.97669096, 0.97616281,\n",
       "        0.97674378]),\n",
       " 'split1_test_score': array([0.97250048, 0.97674337, 0.97683139, 0.97679618, 0.97677858,\n",
       "        0.97757082]),\n",
       " 'mean_test_score': array([0.97423482, 0.97553763, 0.97652351, 0.97674357, 0.97647069,\n",
       "        0.9771573 ]),\n",
       " 'std_test_score': array([1.73433593e-03, 1.20573874e-03, 3.07883802e-04, 5.26107900e-05,\n",
       "        3.07883337e-04, 4.13520394e-04]),\n",
       " 'rank_test_score': array([6, 5, 3, 2, 4, 1], dtype=int32),\n",
       " 'split0_train_score': array([0.97772927, 0.97735955, 0.97774687, 0.97910248, 0.9788208 ,\n",
       "        0.978187  ]),\n",
       " 'split1_train_score': array([0.97186719, 0.97725432, 0.9764621 , 0.97783529, 0.97920848,\n",
       "        0.97801134]),\n",
       " 'mean_train_score': array([0.97479823, 0.97730694, 0.97710448, 0.97846888, 0.97901464,\n",
       "        0.97809917]),\n",
       " 'std_train_score': array([2.93103878e-03, 5.26157490e-05, 6.42387124e-04, 6.33596550e-04,\n",
       "        1.93841583e-04, 8.78330605e-05])}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(alpha=0, hidden_layer_sizes=6,\n",
       "                                     learning_rate_init=0.03, max_iter=1000,\n",
       "                                     solver='sgd'),\n",
       "             param_grid={'hidden_layer_sizes': [(60, 18, 10)],\n",
       "                         'learning_rate_init': [0.003], 'random_state': [5]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "## Why not?\n",
    "# \n",
    "parameters = {'hidden_layer_sizes':[(60,18,10)],\n",
    "            #'learning_rate':['invscaling'],\n",
    "            'learning_rate_init':[.008],\n",
    "            'random_state':[5]}\n",
    "k_fold.n_splits = 5 ## Let's make sure these are getting about the same results.\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([86.45864501]),\n",
       " 'std_fit_time': array([19.61016358]),\n",
       " 'mean_score_time': array([0.02781482]),\n",
       " 'std_score_time': array([0.00113451]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(60, 18, 10)],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[0.003],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_random_state': masked_array(data=[5],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': (60, 18, 10),\n",
       "   'learning_rate_init': 0.003,\n",
       "   'random_state': 5}],\n",
       " 'split0_test_score': array([0.97623344]),\n",
       " 'split1_test_score': array([0.97720171]),\n",
       " 'split2_test_score': array([0.97491308]),\n",
       " 'split3_test_score': array([0.97869718]),\n",
       " 'split4_test_score': array([0.97583627]),\n",
       " 'mean_test_score': array([0.97657633]),\n",
       " 'std_test_score': array([0.00129002]),\n",
       " 'rank_test_score': array([1], dtype=int32),\n",
       " 'split0_train_score': array([0.97911578]),\n",
       " 'split1_train_score': array([0.97870866]),\n",
       " 'split2_train_score': array([0.97744328]),\n",
       " 'split3_train_score': array([0.97858785]),\n",
       " 'split4_train_score': array([0.97857685]),\n",
       " 'mean_train_score': array([0.97848648]),\n",
       " 'std_train_score': array([0.00055715])}"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1211.920559167862\n"
     ]
    }
   ],
   "source": [
    "parameters = {'hidden_layer_sizes':[(80,30,15), (120,45,22)],\n",
    "            #'learning_rate':['invscaling'],\n",
    "            'learning_rate_init':[.005, .002],#,.01,.02],\n",
    "            'random_state':[5]}\n",
    "k_fold.n_splits = 2 ## Let's make sure these are getting about the same results.\n",
    "mlp_grid = GridSearchCV(mlp_6,param_grid=parameters,return_train_score=True, cv=k_fold)\n",
    "\n",
    "start = time.time()\n",
    "mlp_grid.fit(col_transf.fit_transform(X_train),y_train)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'mean_fit_time': array([ 65.08505464, 141.37754381, 154.3692373 , 193.35038435]),\n",
       "  'std_fit_time': array([ 1.42266822, 15.15124643, 89.1627599 ,  9.17383134]),\n",
       "  'mean_score_time': array([0.09056962, 0.12514114, 0.22130871, 0.20579088]),\n",
       "  'std_score_time': array([0.00115216, 0.00059223, 0.01740718, 0.05881989]),\n",
       "  'param_hidden_layer_sizes': masked_array(data=[(80, 30, 15), (80, 30, 15), (120, 45, 22),\n",
       "                     (120, 45, 22)],\n",
       "               mask=[False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_learning_rate_init': masked_array(data=[0.005, 0.002, 0.005, 0.002],\n",
       "               mask=[False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_random_state': masked_array(data=[5, 5, 5, 5],\n",
       "               mask=[False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'hidden_layer_sizes': (80, 30, 15),\n",
       "    'learning_rate_init': 0.005,\n",
       "    'random_state': 5},\n",
       "   {'hidden_layer_sizes': (80, 30, 15),\n",
       "    'learning_rate_init': 0.002,\n",
       "    'random_state': 5},\n",
       "   {'hidden_layer_sizes': (120, 45, 22),\n",
       "    'learning_rate_init': 0.005,\n",
       "    'random_state': 5},\n",
       "   {'hidden_layer_sizes': (120, 45, 22),\n",
       "    'learning_rate_init': 0.002,\n",
       "    'random_state': 5}],\n",
       "  'split0_test_score': array([0.97667336, 0.97691983, 0.97658533, 0.97691983]),\n",
       "  'split1_test_score': array([0.97681379, 0.97721871, 0.9755286 , 0.97695463]),\n",
       "  'mean_test_score': array([0.97674357, 0.97706927, 0.97605697, 0.97693723]),\n",
       "  'std_test_score': array([7.02159588e-05, 1.49442086e-04, 5.28365827e-04, 1.74021569e-05]),\n",
       "  'rank_test_score': array([3, 1, 4, 2], dtype=int32),\n",
       "  'split0_train_score': array([0.98024683, 0.98042288, 0.97913769, 0.97989472]),\n",
       "  'split1_train_score': array([0.9800007 , 0.97996549, 0.97971902, 0.97864512]),\n",
       "  'mean_train_score': array([0.98012377, 0.98019419, 0.97942836, 0.97926992]),\n",
       "  'std_train_score': array([0.00012306, 0.00022869, 0.00029067, 0.0006248 ])},\n",
       " {'mean_fit_time': array([0.0093298 , 0.0092442 , 0.0088768 , 0.00888491]),\n",
       "  'std_fit_time': array([0., 0., 0., 0.]),\n",
       "  'mean_score_time': array([0., 0., 0., 0.]),\n",
       "  'std_score_time': array([0., 0., 0., 0.]),\n",
       "  'param_hidden_layer_sizes': masked_array(data=[(80, 30, 15), (80, 30, 15), (120, 45, 22),\n",
       "                     (120, 45, 22)],\n",
       "               mask=[False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_learning_rate_init': masked_array(data=[0.005, 0.002, 0.005, 0.002],\n",
       "               mask=[False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_random_state': masked_array(data=[5, 5, 5, 5],\n",
       "               mask=[False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'hidden_layer_sizes': (80, 30, 15),\n",
       "    'learning_rate_init': 0.005,\n",
       "    'random_state': 5},\n",
       "   {'hidden_layer_sizes': (80, 30, 15),\n",
       "    'learning_rate_init': 0.002,\n",
       "    'random_state': 5},\n",
       "   {'hidden_layer_sizes': (120, 45, 22),\n",
       "    'learning_rate_init': 0.005,\n",
       "    'random_state': 5},\n",
       "   {'hidden_layer_sizes': (120, 45, 22),\n",
       "    'learning_rate_init': 0.002,\n",
       "    'random_state': 5}],\n",
       "  'split0_test_score': array([nan, nan, nan, nan]),\n",
       "  'mean_test_score': array([nan, nan, nan, nan]),\n",
       "  'std_test_score': array([nan, nan, nan, nan]),\n",
       "  'rank_test_score': array([1, 2, 3, 4], dtype=int32),\n",
       "  'split0_train_score': array([nan, nan, nan, nan]),\n",
       "  'mean_train_score': array([nan, nan, nan, nan]),\n",
       "  'std_train_score': array([nan, nan, nan, nan])}]"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "#results_array = []\n",
    "#results_array.append(mlp_grid.cv_results_)\n",
    "#mlp_grid.cv_results_\n",
    "results_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([131.34892829]),\n",
       " 'std_fit_time': array([34.87962894]),\n",
       " 'mean_score_time': array([0.2677265]),\n",
       " 'std_score_time': array([0.00543774]),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(200, 80)],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[0.005],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_random_state': masked_array(data=[5],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hidden_layer_sizes': (200, 80),\n",
       "   'learning_rate_init': 0.005,\n",
       "   'random_state': 5}],\n",
       " 'split0_test_score': array([0.97678779]),\n",
       " 'split1_test_score': array([0.97781768]),\n",
       " 'split2_test_score': array([0.97691922]),\n",
       " 'mean_test_score': array([0.9771749]),\n",
       " 'std_test_score': array([0.00045767]),\n",
       " 'rank_test_score': array([1]),\n",
       " 'split0_train_score': array([0.98118439]),\n",
       " 'split1_train_score': array([0.98015449]),\n",
       " 'split2_train_score': array([0.97841185]),\n",
       " 'mean_train_score': array([0.97991691]),\n",
       " 'std_train_score': array([0.00114428])}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "mlp_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_100_40 = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,40),\n",
    "    activation = 'relu',\n",
    "    max_iter = 1000,\n",
    "    learning_rate_init = .03,\n",
    "    alpha = 0\n",
    ")\n",
    "\n",
    "mlp_100_40_pipe = Pipeline(\n",
    "    [('column_transformer', col_transf),\n",
    "    ('mlp_clf', mlp_100_40)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0506\n",
      "Dummy score: 0.9016\n",
      "Train: 0.9465683124565373\n",
      "Test: 0.9494\n",
      "Train Cofusion:\n",
      "[[107533      0]\n",
      " [  6070      0]]\n",
      "Cofusion:\n",
      "[[4747    0]\n",
      " [ 253    0]]\n"
     ]
    }
   ],
   "source": [
    "mlp_100_40_pipe.fit(X_train,y_train)\n",
    "performance_summary(mlp_100_40_pipe,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}